
Rapid Quality Assurance with Requirements Smells


Henning Femmera,∗, Daniel Méndez Fernándeza, Stefan Wagnerb, Sebastian Edera


aSoftware & Systems Engineering, Technische Universität München, GermanybInstitute of Software Technology, University of Stuttgart, Germany


Abstract


Context: Bad requirements quality can cause expensive consequences during the software developmentlifecycle, especially if iterations are long and feedback comes late. Objectives: We aim at a light-weightstatic requirements analysis approach that allows for rapid checks immediately when requirements are writtendown. Method: We transfer the concept of code smells to Requirements Engineering as Requirements Smells.To evaluate the beneﬁts and limitations, we deﬁne Requirements Smells, realize our concepts for a smelldetection in a prototype called Smella and apply Smella in a series of cases provided by three industrial anda university context. Results: The automatic detection yields an average precision of 59% at an averagerecall of 82% with high variation. The evaluation in practical environments indicates beneﬁts such as anincrease of the awareness of quality defects. Yet, some smells were not clearly distinguishable. Conclusion:Lightweight smell detection can uncover many practically relevant requirements defects in a reasonablyprecise way. Although some smells need to be deﬁned more clearly, smell detection provides a helpful meansto support quality assurance in Requirements Engineering, for instance, as a supplement to reviews.


Keywords:Requirements Engineering, Quality Assurance, Automatic Defect Detection, RequirementsSmells


Contents


1Introduction2


2Related work32.1The notion of smells in software engi-3


2.2Quality assurance of software require-ments3


2.35


3Requirements Smells83.1Requirements Smell terminology. . .8


3.2Requirements Smells based on ISO 291489


4Smella: A prototype for RequirementsSmell detection10


4.1Requirements parsing11


4.211


4.312


4.4Findings presentation12


5Requirements Smell detection in theprocess of quality assurance15


6Evaluation166.1Case study design16


6.1.116


6.1.2Case and subjects selection . .17


6.1.3Data collection procedure . . .17


6.1.418


6.1.519


6.219


6.2.1Case and subjects description .20





arXiv:1611.08847v1  [cs.SE]  27 Nov 2016


6.2.2RQ 1: How many RequirementsSmells are present in the artifacts? 21


6.2.3RQ 2.1: How accurate is thesmell detection?25


6.2.4RQ 2.2: Which of these smellsare practically relevant in which29


6.2.5RQ 3:Which requirementsquality defects can be detected30


6.2.6RQ 4: How could smells help33


6.2.7Evaluation of validity34


7Conclusion357.135


7.2Relation to existing evidence36


7.336


7.437


7.537


Appendix ARequirements Checklist 41


1. Introduction


Defects in requirements, such as ambiguities or in-complete requirements, can lead to time and costoverruns in a project [56].Some of the issues re-quire speciﬁc domain knowledge to be uncovered. Forexample, it is very diﬃcult to decide whether a re-quirements artifact is complete without domain knowl-edge. Other issues, however, can be detected moreeasily: If a requirement states that a sensor shouldwork with suﬃcient accuracy without detailing whatsuﬃcient means in that context, the requirement isvague and consequently not testable. The same holdsfor other pitfalls such as loopholes: Phrasing that acertain property of the software under developmentshould be fulﬁlled as far as possible leaves room forsubjective (mis-)interpretation and, thus, can havesevere consequences during the acceptance phase of aproduct [24, 33].To detect such quality defects, quality assuranceprocesses often rely on reviews. Reviews of require-ments artifacts, however, need to involve all relevant


stakeholders [65], who must manually read and un-derstand each requirements artifact. Moreover, theyare diﬃcult to perform. They require a high domainknowledge and expertise from the reviewers [65] andthe quality of their outcome depends on the quality ofthe reviewer [75]. On top of all this, reviewers couldbe distracted by superﬁcial quality defects such as theaforementioned vague formulations or loopholes. Wetherefore argue that reviews are time-consuming andcostly.Therefore, quality assurance processes would beneﬁtfrom faster feedback cycles in requirements engineer-ing (RE), which support requirements engineers andproject participants in immediately discovering cer-tain types of pitfalls in requirements artifacts. Suchfeedback cycles could enable a lightweight qualityassurance, e.g., as a complement to reviews.Since requirements in industry are nearly exclu-sively written in natural language [58] and naturallanguage has no formal semantics, quality defects inrequirements artifacts are hard to detect automati-cally. To face this challenge of fast feedback and theimperfect knowledge of a requirement’s semantics, wecreated an approach that is based on what we callRequirements (Bad) Smells. These are concrete symp-toms for a requirement artifact’s quality defect forwhich we enable rapid feedback through automaticsmell detection.In this paper, we contribute an analysis of whetherand to what extent Requirements Smell analysis cansupport quality assurance in RE. To this end, we


1. deﬁne the notion of Requirements Smells and in-tegrate the Requirements Smells1 concept into ananalysis approach to complement (constructiveand analytical) quality assurance in RE,


2. present a prototypical realization of our smelldetection approach, which we call Smella, and


3. conduct an empirical investigation of our ap-proach to better understand the usefulness of


1In context of our studies, we use the ISO/IEC/IEEE29148:2011 standard [33] (in the following: ISO 29148) asbasis for deﬁning requirements quality. The standard suppliesa list of so-called Requirements Language Criteria, such asloopholes or ambiguous adverbs, which we use to deﬁne eightsmells (see also the smell deﬁnition in Sect. 3.2).


2


a Requirements Smell analysis in quality assur-ance.Our empirical evaluation involves three industrialcontexts: The companies Daimler AG as a represen-tative for the automotive sector, Wacker Chemie AGas a representative for the chemical sector, and Tech-Divison GmbH as an agile-specialized company. Wecomplement the industrial contexts with an academicone, where we apply Smella to 51 requirements arti-facts created by students. With our evaluations, weaim at discovering the accuracy of our smell analysistaking both a technical and a practical perspectivethat determines the context-speciﬁc relevance of thedetected smells. We further analyze which require-ments quality defects can be detected with smells, andwe conclude with a discussion of how smell detectioncould help in the (industrial) quality assurance (QA)process.


Previously published material


This article extends our previously published work-shop paper [24] in the following aspects: We providea richer discussion on the notion of RequirementsSmell and give a precise deﬁnition. We introduceour (extended) tool-supported realization of our smellanalysis approach and outline its integration into theQA process. We extend our ﬁrst two case studies withanother industrial one as well as with an investigationin an academic context to expand our initial empiricalinvestigations by


1. investigating the accuracy of our smell detectionincluding precision, recall, and relevance from apractical perspective,


2. analyzing which quality defects can be detectedwith smells and


3. gathering practitioner’s feedback on how theywould integrate smell detection in their QA pro-cess considering both formal and agile processenvironments.


Outline


The remainder of this paper is structured as follows.In Sect. 2, we describe previous work in the area. InSect. 3, we deﬁne the concept of Requirements Smellsand describe how we derived a set of Requirements


Smells from ISO 29148. We introduce the tool real-ization in Sect. 4 and discuss the integration of smelldetection in context of quality assurance in Sect. 5.In Sect. 6, we report on the empirical study that weset up to evaluate our approach, before concludingour paper in Sect. 7.


2. Related work


In the following, we discuss work relating to theconcept of natural language processing and smells ingeneral, followed by quality assurance in RE, beforecritically discussing currently open research gaps.


2.1. The notion of smells in software engineeringThe concept of code smells was, to the best of ourknowledge, ﬁrst proposed by Fowler and Beck [27]to answer the question at which point the quality ofcode is so low that it must be refactored. Accordingto Fowler and Beck, the answer cannot be objectivelymeasured, but we can look for certain concrete, visiblesymptoms, such as duplicated code [27] as an indicatorfor bad maintainability [35]. This concept of smells,as well as the list that Fowler and Beck proposed, ledto a large ﬁeld of research. Zhang et al. [76] providean in-depth analysis of the state of the art in codesmells. The metaphor of smells as concrete symptomshas since then been transferred to quality of otherartifacts including (unit) test smells [72] and smells forsystem tests in natural language [31]. Ciemniewskaet al. [12], further characterize diﬀerent defects of usecases through the term use case smell. In our work,we extend the notion of smells to the broader contextof requirements engineering and introduce a concretedeﬁnition for the term Requirements Smell.


2.2. Quality assurance of software requirementsThe concept of Requirements Smells is located inthe context of RE quality assurance (QA), which isperformed either manually or automatically.


Manual QA. Various authors have worked on QA forsoftware requirements by applying manual techniques.Some put their focus on the classiﬁcation of qualityinto characteristics [15], others develop comprehen-sive checklists [2, 6, 50, 39, 38]. Regarding QA, some


3


develop constructive QA approaches, such as creatingnew RE languages, e.g. [17], to prevent issues up front,others develop approaches to make analytic QA, suchas reviews, more eﬀective [69]. In a recent empiricalstudy on analytical QA, Parachuri et al. [60] manu-ally investigate the presence of defects in use cases.To sum it up, these works on manual QA provideanalytical and constructive methods, as well as (vary-ing) lists for defects. They strengthen our conﬁdencethat today’s requirements artifacts are vulnerable toquality defects.


Automatic QA. Various publications discuss the au-tomatic detection of quality violations in RE. Wesummarize existing approaches and tools, their pub-lications, and empirical evaluations in Table 2. Wealso created an in-depth analysis of in total 27 relatedpublications evaluating which quality defects or smellsthe approaches opt for in their described detection.In the following, we will ﬁrst explain two related ar-eas (automatic QA for redundancy and for controlledlanguages), before discussing automatic QA for ambi-guity in general. For ambiguity, we ﬁrst describe thoseapproaches that conducted empirical evaluations ofprecision or recall of quality defects related, but notidentical to, the ones of ISO 29148. Afterwards, we fo-cus on publications that mention the same criteria asin the ISO 29148 (see Table 1 for this list and their re-spective empirical evaluations) and discuss the chosenapproaches and results. We publish the complete listof each quality defect that is detected by each of the27 papers, as well as the precision and recall (whereprovided), online as supplementary material [25].


Automatic QA for redundancy. One speciﬁc area ofQA is avoiding redundancy and cloning. WhereasJuergens et al. [34] use ConQAT to search for syn-tactic identity resulting from a copy-and-paste reuse,Falessi et al. [21] aim at detecting similar content,therefore using methods from information retrieval(such as Latent Semantic Analysis [52]).Rago etal. [62] extend this work speciﬁcally for use cases.Their tool, ReqAlign, classiﬁes each step with a se-mantic abstraction of the step. These publicationsanalyze the performance of their approaches, and de-


pending on the artifact and methods achieve precisionand recall close to 1 (see Table 2).


Automatic QA for controlled languages. Another spe-ciﬁc area is the application of controlled language andthe QA of controlled language. RETA [4] speciﬁcallyanalyzes requirements that are written via certainrequirements patterns (such as with the EARS tem-plate [54]). Their goal is to detect both conformanceto the template but also some of the ambiguities as de-ﬁned by Berry et al [7]. The authors report on a casestudy where they look at the template conformancein depth, indicating that template conformance canbe classiﬁed with various NLP suites to a high accu-racy (Precision > 0.85, Recall > 0.9), both with andwithout glossaries. However, the performance of am-biguity detection (such as the detection of pronouns)is not further discussed in the publication. Similarly,AQUSA [51] analyzes requirements written in userstory format (c.f. [14] for a detailed introduction intouser stories), and detects various defects, such as miss-ing rationales, where they achieve a precision of 0.63-1.Circe [1, 29] is a further tool that assumes that re-quirements are written in such requirements patternsand detects violations of context- and domain-speciﬁcquality characteristics by building logical models. Theauthors report on six exemplary ﬁndings, which weredetected in a NASA case study. However, despitetheir value to automatic QA, such approaches requirevery speciﬁc requirements structure.


Automatic QA for ambiguity in general. The remain-ing approaches listed in Table 2 aim at detectingambiguities in unconstraint natural language. Sincethe quality defects detected by the approaches byCiemniewska et al. [12], Kof [45], HeRA by Knauss etal. [41, 42], Kiyavitskaya et al. [40], RESI by Körneret al. [46, 47, 48], and Alpino by DeBruijn et al. [16]are not the ones discussed in ISO 29148 and since wecould not ﬁnd an evaluation of precision and recall ofthese approaches, we omit discussing these approachesin-depth here. An analysis of what these approachesfocus on in detail as well as their evaluation can befound in short in Table 2 and in full length in our sup-plementary material online [25]. In the following, weﬁrst report on those publications that focus on criteria


4


diﬀerent from ISO 29148, but which report precisionor recall. Afterwards, we describe publications thataim at detecting quality violations of ISO 29148 (seeTable 1).First, Chantree et al. [11] target the speciﬁc gram-matical issue of coordination ambiguity (detectingproblems of ambiguous references between parts of asentence), mostly through statistical methods, suchas occurrence and co-occurrence of words. In a casestudy, they report on a precision of their approachmostly between 54% and 75%.even though theydo not explicitly diﬀerentiate between the detectedambiguities and the concept of pronouns. Second,Gleich et al. [30] base their approach on the ambigu-ity handbook, as deﬁned by Berry et al. [7], as wellas company-speciﬁc guidelines. They compare theirdictionary- and POS-based approach against a goldstandard which they created by letting people high-light ambiguities in requirements sentences. The goldstandard deviates substantially, however, from what isconsidered high quality in their guidelines. Therefore,they create an additional gold standard, mostly basedon the guideline rules. Consequently, their precision2


varies between 34% for the pure experts opinion, to97% for a more guideline-based gold standard. Third,Krisch and Houdek [49], focus on the detection ofpassive voice and so-called weak words. They presenttheir dictionary- and POS-based approach to practi-tioners and ﬁnd many false positives, similar to ourRQ 3. In average, a precision of 12% is reported forthe weak words detection. These approaches focus onvery related, but not identical quality violations orsmells.


Automatic QA for ISO 29148 criteria. Lastly, wespeciﬁcally focus on those approaches that reportto detect the criteria from the ISO 29148 standard.Table 1 provides an overview of these works and theirrespective evaluations.


2Gleich et al. calculate their metrics based on the combina-tion of all ambiguities; unfortunately, they do not diﬀerentiate,e.g. by the type of ambiguity. Also, to our knowledge, thegold standard does not diﬀerentiate between the types. Thisprevents a direct comparison to their work.


The ARM tool [74] deﬁnes quality in terms of the(now superseeded) IEEE 830 standard [32] and pro-poses generic metrics, instead of giving feedback di-rectly to requirements engineers. The metrics arecalculated through counting how often a set of pre-deﬁned terms (per metric) occurs in a document, in-cluding a metric of what we call Loopholes. Eventhough they report on a case study with 46 speciﬁ-cations from NASA, only a quantitative overview isreported3. The QuARS tool [19, 18] is based on theauthor’s experience. Bucchiarone et al. [10] describethe use of QuARS in a case study with Siemens andshow some exemplary ﬁndings. SyTwo [22] adoptsthe quality model of QuARS and applies it to usecases. Loopholes and Subjectivity are part of theQuARS quality model. Also RQA is built on a dif-ferent, proprietary quality model, as described byGénova et al. [28], which includes negative terms aswell as pronouns as quality defects. These works alsobuilt upon extending natural language with NLP an-notations, such as POS tags and searching throughdictionaries for certain problematic phrases. However,we could not ﬁnd a detailed empirical investigationof these tools, e.g. with regards to precision and re-call. SREE is an approach by Tjong and Berry [70],which aims at detection of ambiguities with a recallof 100%. Therefore, they completely avoid all NLPapproaches (since they come with imprecision), andbuild large dictionaries of words. The tool includesdetection of loopholes, as well as pronouns; however,they report only on an aggregated precision for allthe diﬀerent types of ambiguities (66-68%) from twocase studies. In our previous paper [24], we searchedfor violations of ISO 29148, yet we provided only aquantitative analysis, as well as qualitative examples.As mentioned before, RETA also issues warnings forpronouns, however, the evaluation in their paper [4]focusses on template conformance.


2.3. Discussion


Previous work has led to many valuable contribu-tions to our ﬁeld. To explore open research gaps, wenow critically reﬂect on previous contributions from


3See also our RQ 1 in Sect. 6.


5


Table 1: Related work on criteria of ISO-29148 standard, detailed supplementary material can be foundonline [25]


ARMQuARSRQASREESmellaRETA[74][19][18][22][10][28][70][24][4]


Ambiguous Adv. & Adj.E/QComparativesE/QLoopholes (or Options)QE/QE/QEEQ*/P*E/QNegative TermsOE/QNon-Veriﬁable TermsE/QPronounsOQ*/P*E/QOSubjectivityE/QE/QEEE/QSuperlativesE/Q


Legend: O=No empirical analysis, E=Examples from Case, Q=Quantiﬁcation, P=Precision analyzed,R=Recall analyzed, *=Aggregated over multiple smells


an evaluation, a quality deﬁnition and a technicalperspective.


First, one gap in existing automatic QA approach-es is the lack of empirical evidence, especially underrealistic conditions. Only few of the introduced contri-butions were evaluated using industrial requirementsartifacts.Those who do apply their approach onsuch artifacts focus on quantitative summaries ex-plaining which ﬁnding was detected and how oftenit was detected. Some authors also give examples ofﬁndings, but only few works analyze this aspect indepth with precision and recall, especially in the fuzzydomain of ambiguity (see Table 2). When looking atthe characteristics that are described in ISO 29148,we have not seen a quantitative analysis of precisionand recall. Furthermore, reported evidence does notinclude qualitative feedback from engineers who aresupposed to use the approach, which could revealmany insights that cannot be captured by numbersalone. However, we postulate that the accuracy ofquality violations very much depends on the respectivecontext. This is especially true for the fuzzy domain ofnatural language where it is important to understandthe (context-speciﬁc) impact of a ﬁnding to rate itsdetection for appropriateness and eventually justifyresolving the issue.


Second, the existing approaches are based on pro-prietary deﬁnitions of quality, based on experienceor, sometimes, simply on what can be directly mea-sured. The ARM tool [74] is loosely based on the


IEEE 830 [32] standard. However, as the recent liter-ature survey by Schneider and Berenbach [67] states:“the ISO/IEC/IEEE 29148:2011 is actually the stan-dard that every requirements engineer should be fa-miliar with”. We are not aware of an approach thatevaluates the current ISO 29148 standard [33] in thisrespect. As Table 1 shows, for most language qualitydefects of ISO 29148, there has not yet been a toolto detect these quality defects. To all our knowledge,for neither of these factors, there is an diﬀerentiatedempirical analysis of precision and recall. Yet, manyother quality models (most notably from the ambigu-ity handbook by Berry et al. [7]) and quality violationscould lead to Requirements Smells, as far as they com-ply with the deﬁnition given in the next section.


Finally, taking a more technical perspective, ourRequirements Smell detection approach does not fun-damentally diﬀer from existing approaches. Similarto previous works, we apply existing NLP techniques,such as lemmatization and POS tagging, as well asdictionaries. For the rules of the ISO 29148 standard,no parsing or ontologies (as used in other approaches)were required. However, to detect superlatives andcomparatives in German, we added a morphologicalanalysis, which have not yet seen in related work.


In summary, in our contribution, we extend thecurrent state of reported evidence on automatic QAfor requirements artifacts via systematic studies interms of distribution, precision, recall, and relevance,as well as by means of a systematic evaluation with


6


Table 2: Related approaches and tools, and their evaluation, detailed supplementary material can be found online [25]


Tool/ApproachPurpose (unless ambiguity det.)PublicationsEvaluationPrecisionRecall


ConQATRedundancy[34]E/Q/P0.27-1–(Falessi)Redundancy[21]Q/P/Rup to 96up to 96ReqAlignRedundancy[62]Q/P/R0.630.86


RETAStructured Language Rules[4]E/Q/P/R0.85-0.940.91-1AQUSAUser Story Rules[51]E/Q/P0.63-1–CIRCEStructured Language Rules[29] [1]E––


(Ciemniewska)[12]E––(Kof)[44]E/Q––(Kiyavitskaya)[40]E/Q––RESI[46] [47] [48]E/Q––HeRA[41] [42]E––Alpino[16]E/Q––


(Chantree)[11]E/P/R0.6-10.02-0.58Gleich[30]E/Q*/P*/R*0.34-0.970.53-0.86(Krisch)[49]E/Q/P0.12–


ARMRE Artifact Metrics[74]Q––QuARS / SyTwo[19] [18] [22] [10]E/Q––RQA[28]O––SREE[70]Q*/P*0.66-0.68*–Smella[24]E/Q––


Legend: O=No empirical analysis, E=Examples from Case, Q=Quantiﬁcation, P=Precision analyzed, R=Recall analyzed,*=Aggregated over multiple smells


7


practitioners under realistic conditions. We performthis on both existing, as well as new quality defectstaken from the ISO 29148. Therefore, we extend ourpreviously published ﬁrst empirical steps [24] to closethese gaps by thorough empirical evaluation.


3. Requirements Smells


We ﬁrst introduce the terminology on RequirementsSmells as used in this paper. In a second step, wedeﬁne those smells we derived from ISO 29148 andwhich we use in our studies, before describing the toolrealization in the next section.


3.1. Requirements Smell terminology


Code smells are supposed to be an imprecise in-dication for bad code quality [27].We apply thisconcept of smells to requirements and deﬁne it asfollows: A Requirements Smell is an indicator of aquality violation, which may lead to a defect, with aconcrete location and a concrete detection mechanism.In detail, we consider a smell as having the followingcharacteristics:


1. A Requirements Smell is an indicator for a qual-ity violation of a requirements artifact. For thisdeﬁnition, we understand requirements quality interms of quality-in-use, meaning that bad require-ments artifact quality is deﬁned by its (potential)negative eﬀects on activities in the software life-cycle that rely on these requirements artifacts(see also [26]).


2. A Requirements Smell does not necessarily leadto a defect and, thus, has to be judged by the con-text (supported e.g. by (counter-/)indications).Whether a Requirements Smell ﬁnding is or isnot a problem in a certain context must be indi-vidually decided for that context and is subjectto reviews and other follow-up quality assuranceactivities.


3. A Requirements Smell has a concrete locationin an entity of the requirements artifact itself,e.g. a word or a sequence. Requirements Smellsalways provide a pointer to a certain location that


QA must inspect. In this regard, it diﬀers fromgeneral quality characteristics, e.g. completeness,that only provide abstract criteria.


4. A Requirements Smell has a concrete detectionmechanism. Due to its concrete nature, Require-ments Smells oﬀer techniques for detection of thesmells. These techniques can, of course, be moreor less accurate.


Furthermore, we deﬁne a quality defect as a concreteinstance or manifestation of a quality violation in theartifact, in contrast to a ﬁnding which is an instanceof a smell. However, like a smell indicates for a qualityviolation, the ﬁnding indicates for a defect. Fig. 1visualizes the relation of these terms.


Instance


Quality Model


Requirements Smells


indicates for


RE 


Entity


supported by


indicates for


detects


automated by


present in


instance of


decreases


FindingQuality 


Defect


SmellQuality 


Violation


Quality-in-


use


Smell 


Detector


Figure 1: Terminology of Requirements Smells (sim-pliﬁed)


In the following, we will focus on natural languageRequirements Smells, since requirements are mostlywritten in natural language [58]. Furthermore, the realbeneﬁts of smell detection in practice should comewith automation. Therefore, the remainder of thepaper discusses only Requirements Smells where thedetection mechanism can be executed automatically(i.e. it requires no manual creation of intermediate orsupporting artifacts).


8


3.2. Requirements Smells based on ISO 29148


We develop a set of Requirements Smells based onan existing deﬁnition of quality. For the investiga-tions in scope of this paper, we take the ISO 29148requirements engineering standard [33] as a baseline.The reasons for this are two-fold.First, the ISO 29148 standard was created to har-monize a set of existing standards, including theIEEE 830:1998 [32] standard. It diﬀerentiates betweenquality characteristics for a set of requirements, suchas completeness or consistency, and quality character-istics for individual requirements, such as unambiguityand singularity. The standard furthermore describesthe usage of requirements in diﬀerent project phasesand gives exemplary contents and structure for re-quirements artifacts. Therefore, we argue that thisstandard is based on a broad agreement and accep-tance. Recent literature studies come to the sameconclusion [67].Second, the standard provides readers with a listof so-called requirements language criteria which sup-port the choice of proper language for requirementsartifacts. The authors of the standard argue thatviolating the criteria results “in requirements that areoften diﬃcult or even impossible to verify or may allowfor multiple interpretations" [33, p.12]. For deﬁningour smells, which we describe next, we refer to thissection of the standard and use all the deﬁned require-ments language criteria. We employ those criteriaas a starting point and deﬁne the smells by addingthe aﬀected entities (e.g. a word) and an explana-tion. Here, we do not discuss the impact smells haveon the quality-in-use. Essentially, smells hinder theunderstandability of requirements and consequentlytheir subsequent handling and their veriﬁcation (fora richer discussion, see also previous work in [26]).Our current understanding is based on the ex-amples given by the standard.A subset ofthe language criteria,namelySubjective Lan-guage,Ambiguous Adverbs and Adjectives andNon-verifiable Terms,as deﬁned in [33],arestrongly related, essentially since subjective languageis a special type of ambiguity, which may lead to is-sues during veriﬁcation. Since the intention of thiswork is to start with the standard as a deﬁnition of


quality, in the following, we will remain with the pro-vided deﬁnition based on the language criteria andleave the development of a precise and complete setof Requirements Smells to future work. In detail, weuse the requirements language criteria to derive thesmells summarized next.


Smell Name:Subjective LanguageEntity:WordExplanation:Subjective Language refers towords of which the semantics isnot objectively deﬁned, such asuser friendly, easy to use, costeﬀective.Example:The architecture as well as theprogramming must ensure a sim-ple and eﬃcient maintainabil-ity.


Smell Name:Ambiguous Adverbs and Ad-jectivesEntity:Adverb, AdjectiveExplanation:Ambiguous Adverbs and Adjec-tives refer to certain adverbs andadjectives that are unspeciﬁc bynature, such as almost always, sig-niﬁcant and minimal.Example:If the (...) quality is too low, afault must be written to the errormemory.


Smell Name:LoopholesEntity:WordExplanation:Loopholes refer to phrases thatexpress that the following require-ment must be fulﬁlled only to acertain, imprecisely deﬁned ex-tent.Example:As far as possible, inputs arechecked for plausibility.


9


Smell Name:Open-ended, Non-verifiableTermsEntity:WordExplanation:Open-ended, non-veriﬁable termsare hard to verify as they oﬀer achoice of possibilities, e.g. for thedevelopers.Example:The system may only be acti-vated, if all required sensors (...)work with suﬃcient measure-ment accuracy.


Smell Name:SuperlativesEntity:Adverb, AdjectiveExplanation:Superlatives refer to requirementsthat express a relation of the sys-tem to all other systems.Example:The system must provide the sig-nal in the highest resolution thatis desired by the signal customer.


Smell Name:ComparativesEntity:Adverb, AdjectiveExplanation:Comparatives are used in require-ments that express a relation ofthe system to speciﬁc other sys-tems or previous situations.Example:The display (...)contains theﬁelds A, B, and C, as well asmore exact build infos.


Smell Name:Negative StatementsEntity:WordExplanation:Negative Statements are “state-ments of system capability notto be provided"[33]. Some arguethat negative statements can leadto underspeciﬁcation, such as lackof explaining the system’s reac-tion on such a case.Example:The system must not sign oﬀusers due to timeouts.


Smell Name:Vague PronounsEntity:PronounExplanation:Vague Pronouns are unclear rela-tions of a pronoun.Example:The software must implementservices for applications, whichmustcommunicatewithcon-troller applications deployed onother controllers.


Smell Name:Incomplete ReferencesEntity:Text referenceExplanation:Incomplete References are refer-ences that a reader cannot follow(e.g. no location provided).Example:[1] “Unknown white paper". Pe-ter Miller.


4. Smella:A prototype for RequirementsSmell detection


Requirements Smell detection, as presented in thispaper, serves to support manual quality assurancetasks (see also the next section). The smell detectionis implemented on top of the software quality analysistoolkit ConQAT,4 a platform for source code analysis,which we extended with the required NLP features.In the following, we introduce the process for the au-tomatic part of the approach, i.e. the detection andreporting of Requirements Smells. To the best of ourknowledge, there is no tool, other than the ones men-tioned in related work, that detect and present thesesmells in natural language requirements documents.The process takes requirements artifacts in vari-ous formats (MS Word, MS Excel, PDF, plain text,comma-separated values) and consists of four steps(see also Fig. 2):


1. Requirements parsing of the requirements arti-facts into single items (e.g. sections or rows),resulting in plain texts, one for each item


4http://www.conqat.org


10


Requirements


AnnotationIdentiﬁcationParsing


Spec A1


Spec B1


Sec1Req1Req2Req3


Sec2Req1Req2


Sec1Req1Req2Req3


POS Tagging


Morphologic Analysis


Lemmatization


1234


Presentation


Overview 


Dashboard


Smell 


Viewer


Figure 2: The overall smell detection process


2. Language annotation of the requirements withmeta-information


3. Findings identiﬁcation in the requirements, basedon the language annotations


4. Presentation of a human-readable visualizationof the ﬁndings as well as a summary of the results


The techniques behind these steps are explained inthe following section.


4.1. Requirements parsing


Our current tool is able to process several ﬁle for-mats: MS Word, MS Excel, PDF, plain text andcomma-separated values (CSV). Depending on theformat, the ﬁles are parsed in diﬀerent ways. Plaintext and PDF are taken as is and parsed ﬁle by ﬁle.Microsoft Word ﬁles are grouped by their sections.For Microsoft Excel and CSV ﬁles, we deﬁne thosecolumns that represent the IDs or names (if there areany), and those columns should be used as text inputto detect smells.If a ﬁle is written in a known template, such as acommon template for use cases, we can make use ofthis template to understand structural defects, suchas lacking content items in a template. In the remain-der of this paper, however, we focus on the naturallanguage Requirements Smells as provided by the ISOstandard.


4.2. Language annotationFor the annotation and smell detection steps, weemploy three techniques from Natural Language Pro-cessing (NLP) [36]. Table 3 additionally shows whichof the techniques we use for which smell.


POS Tagging: For two smells, we use part-of-speech(POS) tagging. Given a sentence in natural lan-guage, it determines the role and function ofeach single word in the sentence. The output isa so-called tag for each word indicating, for in-stance, whether a word is an adjective, a particle,or a possessive pronoun. We used the StanfordNLP library [71] and the RFTagger [66] for this.Both are statistical, probabilistic taggers thattrain models similar to Hidden Markov Modelsbased on existing databases of tagged texts. Adetailed introduction into the technical details ofPOS tagging is beyond the scope of this paperbut can be found, for example, in [36]. We usePOS tagging to determine so-called substitutingpronouns. These are pronouns that do not re-peat the original noun and, thus, need a human’sinterpretation of its dependency.


Morphological Analysis: Based on POS tagging,we perform a more detailed analysis of text anddetermine a word’s inﬂection. This includes, interalia, determining a verb’s tense or an adjective’scomparison. We use this technique to analyze if


11


adjectives or adverbs are used in their compara-tive or superlative form.


Dictionaries & Lemmatization: For the remain-ing ﬁve smells, we use dictionaries based on theproposals of the standard [33] and on our ex-periences from ﬁrst experiments in a previouswork [24]. We furthermore apply lemmatizationfor these words, which is a normalization tech-nique that reproduces the original form of a word.In other words, if a lemmatizer is applied to thewords were, is or are, the lemmatizer will re-turn for all three the word be. Lemmatizationis in its purpose very similar to stemming (see,e.g. the Porter Algorithm [61]), yet not basedon heuristics but on the POS tag as well as theword’s morphological form. For RequirementsSmells, the diﬀerence is signiﬁcant: For example,the words use and useful stem to the same wordorigin (use), but to diﬀerent lemmas (i.e. mean-ings; use and useful). Whereas the lemma use ismostly clear to all stakeholders, the lemma usefulis easily misinterpreted.


4.3. Findings identiﬁcation


Based on the aforementioned information, we iden-tify ﬁndings. This step actually ﬁnds the parts of anartifact that exhibit bad smells. Dependent on theactual smell, we use diﬀerent techniques, as shown inTable 3. If the smell relates to a grammatical aspect,we search through the information from POS taggingand morphological analyses. For example, for theSuperlatives Smell, we report a ﬁnding if an adjec-tive is, according to morphologic analysis, inﬂectedin its superlative form. If the smell does not relateto grammatical aspects but rather the semantics ofthe requirements, we identify the smell by matchingthe lemma of a word against a set of words from pre-deﬁned dictionaries. Since the requirements underanalysis in our cases did not contain references, in-complete references are not part of our tool at present.


4.4. Findings presentation


We implemented the presentation of ﬁndings ina prototype, which we call Smella (Smell Analysis).


Smella is a web-based tool that enables viewing, re-viewing and blacklisting ﬁndings as well as a hotspotanalysis at an artifact level. In the Smella presenta-tion, we display the complete requirements artifactand annotate ﬁndings in a spell checker style. Thisfollows the idea of smells as only indications that mustbe evaluated in their context. Lastly, the system givesdetailed information when a user hovers a ﬁnding (seeFig. 3). In the following, we shortly describe the fea-tures of Smella in detail to provide the reader with arough understanding of the prototype.


View ﬁndings: At the level of a single artifact, wepresent the text of the artifact and its structure.We mark all ﬁndings in the text. With a click onthe markers, more information about the ﬁndingis displayed. The tool provides an explanation ofthe rationale behind this smell and possible im-provements for the ﬁnding depending on the smell(every smell has a message for improvements).


Review ﬁndings: We allow the user to write a re-view and to set a status for each ﬁnding, bothsupporting feedback mechanisms within and be-tween project teams. A user has the possibilityto accept or reject a ﬁnding but also to set a cus-tom state, for example under review. Accepting aﬁnding means the ﬁnding needs to be addressed.If a ﬁnding is rejected, the ﬁnding does not needto be addressed. The semantics of the customstatus is open to the reviewer.


Blacklist ﬁndings: Smells are only indicators forissues. Therefore, users can reject ﬁndings. Ifa ﬁnding is rejected by the user, the ﬁnding isremoved from the visualization and will not bepresented to the user anymore.


Disable smells: Often, users are interested in only asubset of smells or even just one smell. Therefore,we allow the user to hide all ﬁndings of particularsmells and to select the smells she wants to displayin the artifact view.


Analyze hotspots: In this view, we present all ar-tifacts in a colored treemap (see Fig. 4). Everybox in the treemap is one artifact, with the color


12


Figure 3: A sample output from the smell detection tool (detailed artifact view) with some smells disabledand some ﬁndings blacklisted


13


Figure 4: A sample output from the smell detection tool (hotspot analysis view)


14


Table 3: Detection techniques for smells


Smell NameDetection Mechanism


Subjective LanguageDictionaryAmbiguous Adverbs and AdjectivesDictionaryLoopholesDictionaryOpen-ended, non-veriﬁable termsDictionarySuperlativesMorphological analysis or POS taggingComparativesMorphological analysis or POS taggingNegative StatementsPOS tagging and dictionaryVague PronounsPOS tagging: Substituting pronouns.Incomplete ReferencesNot in scope of this study


of the box indicating the number of ﬁndings: themore red an artifacts is, the more ﬁndings it con-tains (the more it “smells” bad). The artifactsare grouped by their folder structure. The toolprovides a summarized treemap for all smellsas well as a separate treemap for all individualsmells. With these treemaps, users can identifyartifacts or groups of artifacts exhibiting a highnumber of ﬁndings – for one single smell but alsofor all smells together. This feature supports theidentiﬁcation of candidates for in-depth reviews.


5. Requirements Smell detection in the pro-cess of quality assurance


The Requirements Smell detection approach de-scribed in previous sections serves the primary purposeof supporting quality assurance in RE. The detectionprocess itself is, however, not restricted to particu-lar quality assurance tasks, nor does it depend on aparticular (software) process model as we will showin Sect. 6. Hence, a smell detection, similar to thenotion of quality itself, always depends on the viewsin a socio-economic context. Thus, how to integratesmell detection into quality assurance needs to be an-swered according to the particularities of that context.In the following, we therefore brieﬂy outline the rolesmell detection can generally take in the process ofquality assurance. More concrete proposals on howto integrate it into speciﬁc contexts are given in ourcase studies in Sect. 6.


We postulate the applicability of the RequirementsSmell detection in the process of both constructiveand analytical quality assurance (see Fig. 5). Fromthe perspective of a constructive quality assurance,authors can use the smell detection to increase theirawareness of potential smells in their requirementsartifacts and to remove smells before releasing an ar-tifact for, e.g., an inspection. External reviewers inturn, can then use the smell detection to prepare an-alytical, potentially cost-intensive, quality assurancetasks, such as a Fagan inspection [20]. Such an in-spection involves several reviewers and would beneﬁtfrom making potential smells visible in advance. Iter-ative inspection approaches are also known as phasedinspections, as deﬁned by Knight and Myers [43].


We furthermore believe that one major advantageis that the scope of our smell detection is not toenforce resolving a potential smell but to increasethe awareness of the like and to make transparentlater reasoning why certain decisions have been taken.Please note that two diﬀerent roles (e.g. requirementsengineer and QA engineer) can take two diﬀerent view-points on the same smell, respectively its criticalityand whether it should be resolved or not. In addition,a ﬁnding could be unambiguous to the author, butunclear to the target group of readers (represented bythe reviewers). Therefore, one contribution of our tool-supported smell detection is also to actively foster thecommunication between reviewers and authors and toenable continuous feedback between both roles. Forthis reason, we enable stakeholders in Smella to com-


15


Automatic 


Smell Detection


Constructive QAAnalytical QA


Detectsmells


AuthorReviewer


Feedback


View ﬁndings & Review ﬁndings


View ﬁndings & Review ﬁndings


Create / update


Visualizesmells


Figure 5: A suggestion for applying Requirements Smell detection in QA


ment on detected smells and make explicit whetherthey need to be resolved or whether and why theyhave been accepted or rejected.


6. Evaluation


For a better, empirical understanding of smells inrequirements artifacts, we conducted an exploratorymulti-case study with both industrial and academiccases. We particularly rely on case study researchover other techniques, such as controlled experiments,because we want to evaluate our approach in practicalsettings under realistic conditions. For the designand reporting of the case study, we largely follow theguidelines of Runeson and Höst [63].


6.1. Case study design


Our overall research objective is as follows:


Research Objective: Analyze whether automaticanalysis of Requirements Smells helps in requirements


artifact quality assurance.


To reach this aim, we formulate four research ques-tions (RQ). In the following, we introduce those re-search questions, the procedures for the case andsubjects selection, the data collection and analysis,and the validity procedures.


6.1.1. Research questionsRQ 1: How many smells are present in require-ments artifacts? To see if the automatic detectionof smells in requirements artifacts could help in QA,we ﬁrst need to verify that Requirements Smells existin the real world. The answer to this question fostersthe understanding how widespread the smells underanalysis are in industrial and academic requirementsartifacts.RQ 2: How many of these smells are relevant?Not only the number of detected smells is important.If many of the detected smells are false positives andnot relevant for the requirements engineers and devel-opers, it would hinder QA more than it would help.


16


As relevancy is a rather broad concept, we break downRQ 2 into two sub-questions.RQ 2.1: How accurate is the smell detec-tion? The ﬁrst sub-question looks at the moretechnical view on relevance. We want to ﬁndfalse positives and false negatives to determinethe precision and recall of the analysis in termsof correct detection of the deﬁned smell.RQ 2.2: Which of these smells are practi-cally relevant in which context? This sec-ond sub-question is concerned with practical rel-evance.We investigate whether practitionerswould react and change the requirement whenconfronted with the ﬁndings.RQ 3: Which requirements quality defects canbe detected with smells? After we understood howrelevant the analyzed Requirements Smells are, wewant to understand their relation to existing qualitydefects in requirements artifacts. Hence, we need tocheck whether, and if so, which defects in requirementsartifacts correspond to smells, as we understand smellﬁndings as indicators for defects.RQ 4: How could smells help in the QA pro-cess? Finally, we collect general feedback from prac-titioners whether (and how) smell detection could bea useful addition to QA for requirements artifacts andwhether as well as how they would integrate the smelldetection into their QA process.


6.1.2. Case and subjects selectionOur case and subject selection is opportunistic butin a way that maximizes variation and, hence, eval-uates the smell detection in very diﬀerent contexts.This is particularly important for investigating re-quirements artifacts under realistic conditions, alsodue to the large variation in how these artifacts man-ifest themselves in practice. A prerequisite for ourselection is the access to the necessary data. To geta reasonable quantitative analysis of the number ofsmells (RQ 1) and qualitative analysis of the rela-tion of smells and defects (RQ 3), we complementour three industrial cases with a case in an academicsetting. There, various student teams are asked toprovide software with a certain set of (identical) func-tionality for a customer as part of a practical course.This is also a realistic setting but provides us with a


higher number of speciﬁcations and reviews than inthe industrial cases.We will refer to the subjects of the industrial casesas practitioners and we will call the latter subjectsstudents.


6.1.3. Data collection procedureWe used a 6-step procedure to collect the datanecessary for answering the research questions.


1. Collect requirements artifact(s) for each case. Weretrieved the requirements artifacts to be ana-lyzed in each case. For one case, the require-ments were stored in Microsoft Word Documents.For the other cases, this involved extracting therequirements from other systems, either a propri-etary requirements management tool (resultingin a list of html ﬁles), or the online task manage-ment system JIRA, which led to a set of comma-separated values ﬁles. For the student projects,the students handed in their ﬁnal artifacts eitheras a single PDF or as a PDF with the generalartifact and another PDF with the use cases.Where authors explicitly structured requirementsin numbered requirements, user stories or usecases, we counted these artifacts.


2. Run the smell detection via Smella. We appliedour detection tool as introduced in Sect. 4.4 onthe given requirements artifacts, which generateda list of smells per artifact.


3. Classify false positives. For all cases in which wewanted to present our results to practitioners, wereviewed each detected ﬁnding. In pairs of re-searchers, we classiﬁed the ﬁndings as either trueor false positive. We classiﬁed a ﬁnding as falsepositive if the ﬁnding was not an instance of thesmell, e.g. because the results of the linguisticanalysis was incorrect.5For artifacts contain-ing more than 10 ﬁndings of a smell, we onlyinspected a set of 10 random ﬁndings (of that


5For example, if the linguistic analysis incorrectly classiﬁedthe word provider in the sentence “As a provider, I want [. . . ]”as a comparative adjective.


17


smell) per artifact. The same holds for Case D,where we inspected 10 random ﬁndings of eachcategory for the whole case.


4. Inspect documents for false negatives.Tocalculate the recall of the smell detection,for each case we randomly selected one ar-tifact that a pair of researchers inspectedfor false negatives.To ease the manualinspection,we grouped the smells Subjec-tive Language,Ambiguous Adverbs and Ad-jectives, Loopholes, Non-verifiable Terms(as Ambiguity-related smells). We classiﬁedwhether a ﬁnding is a true or false negative basedon the same conditions as in the previous step.


One common cause for false negatives fordictionary-based smells can be that an ambigu-ous phrase is not part of the dictionary. Sincewe developed the dictionaries based on existingdictionaries, such as the standard, these dictio-naries are not yet complete and must be furtherdeveloped. However, since this is an issue that isnot a problem of the smell detection approach ingeneral, but rather a conﬁguration task, we didnot take these ﬁndings into consideration for therecall.


5. Get rating by practitioners. We selected a subsetof the true positive ﬁndings so that we cover allsmells with a minimum of two ﬁndings per smellas far as the artifacts allowed. When we foundrepeating or similar ﬁndings, e.g. multiple similarsentences with the same smell, we also includedone of these ﬁndings into the set.


We presented this subset to the practition-ers and interviewed them, ﬁnding by ﬁnding,through three closed questions (see also Table 9):Q1: Would you consider this smell as relevant?Q2: Have you been aware of this ﬁnding before?Q3: Would you resolve the ﬁnding? Of these,the former two must be answered with yes orno. For the last question, we also needed to takethe criticality into account. Therefore, in casepractitioners answered that they would resolve aﬁnding, we also asked whether they would resolveit immediately, in a short time (i.e. within this


project iteration) or in a long time (e.g. if it hap-pens again). In addition to these three questions,we took notes of qualitative feedback, such asdiscussions.


6. Interview practitioners. In addition to the rat-ings, we performed open interviews with prac-titioners about their experience with the smelldetection and how they might include it in theirquality assurance process. We took notes of theanswers.


7. Get review results from students. Lastly, the stu-dents performed reviews of the artifacts of otherstudent teams. They documented and classiﬁedfound problems according to a checklist (see Ta-ble A.11) without awareness of the smell ﬁndingsin their artifacts. We then collected the reviewreports from the students.


6.1.4. Analysis procedureWe structure our analysis procedure into sevensteps. Each step leads to the results necessary foranswering one of our research questions.


1. Calculate ratios of ﬁndings per artifact. To un-derstand whether smells are a common issue inrequirements artifacts, we compared the quanti-tative summaries of smells in the various artifactsand domains. To enable a comparison betweendiﬀerent types of requirement artifacts, we usedthe number of words in each artifact as a measureof size. Hence, we ﬁnally reported the ratio ofﬁndings per 1000 words for each smell and allsmells in total. This provided answers for RQ 1.


2. Calculate ratios of ﬁndings for parts of user sto-ries. In one case, we had a common structure ofthe requirements, because they were formulatedas user stories. To get a deeper insight into thedistribution of smells and ﬁndings, we calculatedthe ratios of ﬁndings per 1000 words for eachpart. We divided the user stories into the partsWe counted the words and ﬁndings in each part.This provided further insights into the answer forRQ 1.


18


3. Calculate ratios of false positives. After a roughoverview obtained under the umbrella of RQ 1describing the number of ﬁndings for each smellof the varying artifacts, we wanted to better un-derstand the smell’s relevance. The ﬁrst stepwas to calculate the ratios of false positive as weclassiﬁed them in Step 3 of the data collection.We reported false positive rates overall and foreach smell. This provides the ﬁrst part of theanswer to RQ 2.1.


4. Calculate ratios of false negatives. The preci-sion of a smell detection is tightly coupled withthe recall. Therefore, we calculated the ratio ofdetected smell ﬁndings to all existing ﬁndings,according to our manual inspection, as describedin Step 4 of the data collection procedure. Thisprovides the second part of the answer to RQ 2.1.


5. Calculate ratio of irrelevant smells. We were notonly interested in errors in the linguistic analysisbut also in how relevant the correct analyseswere for the practitioners. Hence, we calculatedand reported the ratios of ﬁndings consideredirrelevant by the practitioners.This answersRQ 2.2.


6. Compare defects from reviews with ﬁndings. Fromthe students, we received review reports for eachartifact. As the eﬀort to check them all wouldhave been overwhelming, we took a random sam-ple of 20% of the artifacts. For each of the defectsdetected in the review, we checked if there is acorresponding ﬁnding from a smell. This answersRQ 3.


7. Interpret interview notes. To answer ﬁnally RQ 4,we analyze the interview transcripts and code theanswers given by the interviewees manually.


6.1.5. Validity procedureFirst, we used peer debrieﬁng in the sense that alldata collection and analyses were done by at least tworesearchers. Analysis results were also checked by allresearchers. This researcher triangulation especiallyincreases the internal validity. Furthermore, we kept


an audit trail in a Subversion system to capture allchanges to documents and analyses.Second, we performed all the classiﬁcations of ﬁnd-ings into true and false positives in pairs. This alreadyhelped to avoid misclassiﬁcations. To further checkour classiﬁcations, we afterwards did an independentre-classiﬁcation of randomly selected 10% of the ﬁnd-ings and calculated the inter-rater agreement. Wediscussed to clarify which ﬁndings we consider falsepositives and repeated the classiﬁcations until wereached an acceptable agreement. The same proce-dure held for the inspection of artifacts to detect falsenegatives, which we also conducted in pairs. Further-more, we also independently re-classiﬁed one of theartifacts to understand the inter-rater agreement onthe false negatives. Overall, our analysis for false pos-itives and relevance of the ﬁndings is also a validityprocedure in the sense that we check in RQ 2 theresults from RQ 1.Third, we discussed with the practitioners whatrelevance of smells means in the context of the studyto avoid misinterpretations. Furthermore, we gave thestudents review guidelines to give them an indicationwhat quality defects in requirements artifacts mightbe. Both serve in particular as mitigation to threatsto the internal and the construct validity.Fourth, we performed the analysis of the corre-spondence between smells and defects with a pair ofresearchers. This pair derived a classiﬁcation of thefound and not found defects. Both other researchersreviewed the classiﬁcation, and we improved it itera-tively until we reached a joint agreement.Fifth, we performed member checking by showingour transcriptions and interpretations for RQ 4 to theinterviewed practitioners and incorporating feedback.Finally, to support the external validity of the re-sults of our study, we aimed at selecting cases withmaximum variation in their domains, sizes, and howthey document requirements.


6.2. Results


In the following, we report on the results of our casestudies. We ﬁrst describe the cases and subjects underanalysis, before we answer the research questions. Weend by evaluating the validity of the cases.


19


6.2.1. Case and subjects descriptionThe ﬁrst three cases contain requirements producedin diﬀerent industrial contexts: embedded systemsin the automotive industry, business information sys-tems for the chemical domain and agile developmentof web-based systems. While the ﬁrst two representrather classical approaches to Requirements Engineer-ing, the third case applies the concept of user stories,as it is popular in agile software development. Thefourth case is in an academic background and employsboth use cases and textual requirements. Regardingsubject selection, for each industrial case we selectedpractitioners involved in the company, domain andspeciﬁcation. We executed the ﬁndings rating (Step 5)and the interviews regarding the QA process (Step 6)with the same experts, so that their answer in Step 6is based on their experience with practical, real ex-amples. In the following, we describe the cases, aswell as the experts or students for each case. Table 4provides a quantitative overview of the cases.


Case A: Daimler AG. Daimler AG is a multinationalautomotive corporation headquartered in Stuttgart,Germany. At Daimler, we analyzed six diﬀerent re-quirements artifacts (A1–A6) which were written byvarious authors. The requirements artifacts describefunctionality in diﬀerent domains of engine control aswell as driving information. In this case, requirementsare written down in the form of sentences, identiﬁedby an ID. The authors are domain experts who arecoached on writing requirements.The requirements artifacts A1–A6 consist of 323requirements in total (see Table 4). All of the artifactsof Daimler analyzed in our study were created bydomain experts in a pilot phase after a change in therequirements engineering process as part of a softwareprocess improvement endeavour.For RQ 2.2., wereviewed 22 ﬁndings with an external coach who worksas a consultant for requirements engineering and hastightly collaborated with the group for many years.


Case B: Wacker Chemie AG. In the second case, weanalyzed requirements artifacts of business informa-tion systems from Wacker Chemie AG. Wacker is aglobally active company working in the chemical sec-tor and headquartered in Munich, Germany. The


systems that we analyzed fulﬁl company-internal pur-poses, such as systems for access to Wacker buildingsor support systems for document management.We analyzed three Wacker requirements artifactsthat were written by ﬁve diﬀerent authors. At Wacker,functional requirements are written as use cases (in-cluding ﬁelds for Name, Description, Role and Pre-condition) whereas non-functional requirements aredescribed in simple sentences. The artifacts consistedof 53 use cases and 13 numbered requirements (seeTable 4). For the reviews of the ﬁndings in RQ 2.2,we selected 18 ﬁndings and discussed them with theChief Software Architect, who also has several yearsof experience in quality assurance.


Case C: TechDivision. For the third case, we ana-lyzed the requirements of the agile software engineer-ing company TechDivision GmbH. TechDivision hasaround 70 employees, working in 3 locations in Ger-many. They focus mainly on web development, i.e.creating product portals and e-commerce solutions fora variety of companies, as well as web consulting, espe-cially focusing on search engine optimizations. Manyof their products involve customisation of Magento6


or Typo37 frameworks.In their projects, TechDivision follows an agile soft-ware development process using either Scrum [68] orKanban [3] methodologies. For their requirements,TechDivision applies user stories [14], which they writeand manage in Atlassian JIRA8. User stories at Tech-Divison follow the common Connextra format: As a[Role], I want [Feature], so that [Reason]. We willalso follow this terminology here.The systems under analysis consist of two onlineshopping portals, a customer-relationship system anda content-management system, all of which we cannotname for non-disclosure-agreement reasons. In total,we analyzed over 1,000 user stories containing roughly28,000 words. For RQ 2.2, we met with an experiencedScrum Master and a long-term developer, who haveworked on several projects for TechDivision.


6http://www.magento.com7http://www.typo3.org8https://atlassian.com/software/jira


20


Case D: University of Stuttgart. The requirements ofCase D were created by 52 groups of three 2nd-yearstudents each during a compulsory practical course inthe software engineering programme at the Universityof Stuttgart. We removed one artifact, because it wasincorrectly encoded, thus resulting in 51 requirementsartifacts for this analysis.


2000400060008000


Figure 6: Variation of size of requirements artifactsin Case D in words


The resulting requirements artifacts diﬀer vastly instyle; hence, we were unable to count them in terms ofrequirements, but instead only counted the structureduse cases as provided by the authors, and quantiﬁedthe artifacts by word size.The average size of arequirements artifact was 4,471 words (min: 1,425,max: 8,807, see Fig. 6) and contained 19 use cases(min: 6, max: 39), thus creating a set of artifacts ofnearly a quarter of a million words, including morethan 950 use cases.For practical reasons, we could not evaluate eachresearch question in each case: For example, RQ 3depends on the existence of reviews with documentedresults, which is often not existent in practice. Fur-thermore, depending the answers of RQ 4 on the po-tentially less experienced students from Case D wouldintroduce a threat to the validity of our evaluation.


Table 5 shows the mapping between research ques-tions and study objects. The interviews for RQ 2.2and RQ 4 lasted 60 minutes for each Case A and Band 120 minutes for Case C.


Table 5: Study objects usage in research questions


Case


RQ 1: Distribution


RQ 2.1: Precision


RQ 2.1: Recall


RQ 2.2: Relevance


RQ 3: Defect Types


RQ 4: QA Process


A: Daimler✓✓✓✓B: Wacker✓✓✓✓C: TechDivision✓✓✓✓✓D: Univ. of Stuttgart✓✓✓✓


6.2.2. RQ 1: How many Requirements Smells arepresent in the artifacts?Under this research question, we quantify the num-ber of ﬁndings that appear in requirements. Table 6shows the number of ﬁndings for each case, each re-quirements artifact and each smell and also puts thesenumbers in relation to the size of the artifact. Weanalyzed requirements of the size of more than 250kwords, on which the smell detection produced in totalmore than 11k ﬁndings, thus revealing roughly 44ﬁndings per thousand words.Table 6 shows that all requirements artifacts containﬁndings of Requirements Smells. They vary from 5ﬁndings for the smallest9 case (A3) up to 572 for thelargest case (C4). The number of ﬁndings stronglycorrelates with the size of the artifact (see Fig. 7,Spearman correlation of 0.9). Hence, in the remainder,we normalize the number of ﬁndings by the size of theartifact.The artifacts of Daimler have an average of 26 ﬁnd-ings per thousand words, in contrast to 41 for both


9in terms of total number of words


21


G


G


G


G


020004000600080001000012000


0100200300400500


Number of Words in Artifact


Number of Findings in Artifact


A1A2


A3


A4A5


A6


B1


B2B3


C1


C2


C3


C4


D6


D11


D13


D15


D16


D19


D20


D22


D24


D28


D31


D32


D34


D41


D42


D43


D45


D49


G


Daimler


Wacker


TechDivision


Stuttgart


Figure 7: Number of ﬁndings strongly correlates with size of artifact (for readability reasons, for the Stuttgart cases (blue) onlyIDs of less correlating artifacts are displayed).


22


Wacker and TechDivision and 43 for the artifacts pro-duced by the students. Best to analyze the variancewithin a requirements artifact seems Case D, in whichmultiple teams had a similar background and projectsize. Fig. 8 shows the variance between the artifactsof Case D with an average of 44 ﬁndings, a minimumof 26 ﬁndings (D11) and a maximum of 75 ﬁndings(D32) per 1,000 words.


G


G


G


3040506070


Figure 8: Number of ﬁndings per 1,000 words in CaseD


When inspecting the diﬀerent Requirements Smells,we can see that the most common smells are vaguepronouns with 25 ﬁndings per 1,000 words, followedby the negative words smell with 6 ﬁndings andthe loophole smell with 4 ﬁndings.The least of-ten smells are non-verifiable terms with 1 ﬁndingper 1,000 words, and ambiguous adverbs and ad-jectives with 0.25 ﬁndings per 1,000 words. In fact,the most common smell, vague pronouns, appears100 times more often than the ambiguous adverbsand adjectives. To analyze the variance in depth,we again take the students’ artifacts for reference.Fig. 9 shows the relative number of ﬁndings acrossthe projects.


Interpretation. Weinterpretthequantitativeoverview along three variables: projects, contexts andthe diﬀerent Requirements Smells.


Projects When comparing at project level, we seethat Cases A1–A6 (with outlier A5) and C1–C4(with outlier C3) show quite similar numbers.In contrast B1 to B3 vary between 28 and 68ﬁndings per 1,000 words.When looking intothe most extreme outliers B3 and D32, we see asystematic error that creates a large number ofﬁndings: Both projects repeatedly explain whatthe system should10 do instead of what it mustdo. 16 of 19 loopholes ﬁndings in B3 and 29of 37 loophole ﬁndings in D32 root from thisproblem. This can lead to diﬃcult issues in con-tracting as requirements that are phrased witha should are commonly understood as optional(see e.g. RFC2119 [9] for a detailed explanation).


Hence, we could see a surprising consistency intwo of three industrial case studies. The Wackerdata varies, so does the students case. In bothcases, the negative extremes point at issues thatpotentially have expensive consequences.


Context The four cases diﬀer strongly in their con-text: They write down requirements in diﬀer-ent forms, vary in their software developmentmethodology and also produce software for dif-ferent domains. When comparing the ﬁndingsat the domain level, we see that Daimler arti-facts with an average of 26 ﬁndings per thousandwords contain less ﬁndings than both Wacker andTechDivision with 41 ﬁndings and the artifactsproduced by the students with 43 ﬁndings.


Our partners reported that there have been train-ings for the authors of the cases A1–A6 recently,which could explain the diﬀerence. Another rea-son could be the strong focus that the automotivedomain puts on requirements and requirementsquality in contrast to the other domains. Lastly,also the strict process in this domain could be a


10Soll is a German modal verb that is less strict than anEnglish must.


23


G


G


G


G


G


G


G


G


G


G


G


G


G


GG


SubjectiveLoopholesVague P.SuperlativesNegativesComparativesNon−verifiableAmb. Adverbs


01020304050


Figure 9: Variation of smells per 1,000 words in Case D


24


reason for this striking diﬀerence of the Daimlerrequirements. Unsurprisingly, the students’ re-quirements form the lower end of the scale, yetnot by much.


Requirements Smells When comparing the eightsmells, we see a strong variance between the num-ber of ﬁndings, both in absolute as well as relativevalues. A qualitative inspection indicates reasonsfor the most occurring smells. First, the smelldetection for vague pronouns ﬁnds all substi-tuting pronouns in the requirements. Especiallyin German, in many sentences the reference ofthe pronoun can sometimes be derived from gen-der and grammatical case of the word, thus cor-rectly detecting pronouns, but not vague pro-nouns. RQ 2.1 quantiﬁes this issue. Second, themost common indication for loophole ﬁndingsis the aforementioned use of the word should. Wediscuss this case in-depth with practitioners inRQ 2.2. Third, we will also inspect reasons forthe high number of negative words ﬁndings inRQ 2.1 and RQ 2.2.


Answer to RQ 1. The number of ﬁndings in require-ments artifacts strongly correlates with the size ofthe artifact. There are roughly 44 ﬁndings per 1,000words and some contexts show a striking similarityin the number of ﬁndings for their artifacts. In ourcases, the automotive requirements had a lower num-ber of ﬁndings whereas student artifacts containeda higher number of ﬁndings relative to the size ofthe artifacts. The most common ﬁndings are for thesmells loopholes and vague pronouns.


To understand the capabilities of the smell detec-tion, we need to understand precision as metric indi-cating how many of the detected ﬁndings are correct,as well as recall as a metric indicating how many ofthe correct ﬁndings are detected.


Precision. To understand to which extent the num-bers of ﬁndings for certain smells in RQ 1 are causedby the detection mechanism, we inspected a randomsample of 616 ﬁndings by taking equivalent sets of


ﬁndings from each project and manually classifyingwhether the ﬁnding fulﬁlls the smell deﬁnition. Wecould not inspect the same number of ﬁndings of eachsmell for each project, because some projects only hadfew or even no ﬁndings of a certain smell (see numberof ﬁndings per project in Table 6).Table 7 and Fig. 10 show the summary of this anal-ysis: The precision of the detection of the subjec-tive language smell revealed only three false pos-itives in total, thus leading to a precision of 0.96.Non-verifiable words, loophole, and ambiguousadverbs and adjectives smells range between 0.70and 0.81, hence leading to roughly one mistake in foursuggestions. Comparative and superlative smellsrange around 0.5 which would mean that every sec-ond ﬁnding is correct. At the rear end of the listare the negative words and vague pronouns smellswith one correct ﬁnding in three to four suggestions.Across all smells, the precision is between 0.48 (over allinspections) and 0.59, if we take the varying numberof inspected ﬁndings between the smells into account.To understand these numbers, we qualitatively in-spected the false positive classiﬁcations, revealing thefollowing main reasons for false positives:


Grammatical errors in real world language.


The ﬁrst issue that creates false positives is thefact that our study analyzes real world language.Some of the requirements, especially in CaseC, contained a number of grammatical ﬂaws aswell as dialectal phrases, which lead to wrongresults in the automatic morphologic analysisand automatic POS tagging and consequentlyalso to false positives during smell detection.


Vague pronouns. The smell detection for vaguepronouns showed the lowest precision. In thedetection of this smell, we look for substitutingpronouns, which are pronouns where the nounis not repeated after the pronoun11, of which wecharacterize only every fourth ﬁnding as a defect.The reason behind this poor performance, be-sides a number of false positives due to the poorgrammar mentioned before, is the comparably





25


large number of grammatical exponents of theGerman language. In addition to number andthree grammatical genders, the German languagealso has four grammatical cases. Therefore, invarious instances of substituting pronouns, thereis only one grammatical possibility of what thepronoun could refer to.


Findings in conditions. A third reason for falsepositives is that the smell detection, so far, takesvery little context into account. For example, thecomparatives smell aims at detecting require-ments that deﬁne properties of the system rela-tive to other systems or circumstances12. Whensearching for grammatical comparatives in re-quirements, roughly 48% of the cases are of theaforementioned kind. In roughly the same num-ber of cases, however, the comparative describes acondition. For example, if the requirement statesthat if the system takes more than 1 second to re-spond [. . . ], the comparison is not against anothersystem or circumstance but against absolute num-bers. Therefore, in this case, the comparativedoes not indicate a problem (one could even arguethat this is an indicator for good quality).


A similar problem holds for the negativephrases smell: The smell detection aims at re-vealing statements of what the system should notdo. Often, however, the negative is mentionedin conditions. For example, if the requirementsexpress what to do if the user input is not zero[. . . ], the negation relates to a condition and notto a property of the system.


Recall. When analyzing the accuracy of an automaticdetection, we must look not only at precision, butalso at recall, i.e. the ratio of all detected ﬁndings toall defects of a certain type in an artifact. To thisend, we inspected one artifact of each case, in total a


12As discussed in Sect. 3.2, the problem of comparatives inrequirements is validation: How can we understand whethera system fulﬁlls a requirements if that requirement is statedin a relative instead of an absolute way? What if the systemin comparison changes its properties, would this render therequirement suddenly unfulﬁlled?


set of roughly 16,200 words, and manually identiﬁedthe ﬁndings in each artifact. Due to the problems ofdistinguishing the various ambiguity-related smells,we analyzed the recall of these four smells as if itwas one smell, without further diﬀerentiation (seeSection 6.1.3).The manual inspection revealed 200 ﬁndings in thisartifact sample and an average recall of 0.82. Table 8and Fig. 10 show the summary of the results: The com-parison shows a recall between 0.84 and 0.95 for fourof the ﬁve investigated smells. The highest recall wasachieved by the Comparative Requirements Smell,with 0.95, which means that the smell detection missedone in 20 ﬁndings. The ﬁfth smell, with the lowestrecall, is Superlative Requirements Smell with arecall of 0.5. However, this smell is one of the rarestof the smells, as one can also see in the results to RQ1. Therefore our analysis of the recall of this smell isbased on few data points. Hence, we suggest to takethe recall of this smell with care, and suggest thatfuture studies should investigate this issue in moredepth.A further analysis of the false negatives shows thatthe smell detection missed ﬁndings because of impre-cisions in the NLP libraries (i.e. Stanford NLP [71] forLemmatization and POS Tagging and RFTagger [66]for morphologic analysis). For the dictionary-basedsmells, the lemmatization did not correctly deducethe correct lemma, e.g. it did not understand thata certain word was a plural of a lemma.If onlythe lemmatized version of the word, i.e. the singularform, is in the dictionary, then the smell detectordoes not correctly identify the smell. In the false neg-ative cases for the Comparative and SuperlativeRequirements Smell, RFTagger did not correctlyclassify the inﬂection.


Interpretation. The study revealed that the precisionstrongly varies between the diﬀerent smells. Qual-itative analysis provided further insights describednext.We can now explain the high number of ﬁndingsfor vague pronouns in RQ 1. If we assume that aquarter of the ﬁndings are correct, the number ofﬁndings in this category is closer to the remainingsmells. Also, we could see that while there are certain


26


reasons of impreciseness that root from the studyobjects themselves and are, thus, unavoidable, thereis plenty of space for optimization. First, existingtechniques from NLP could be applied to improvecertain smells, such as the vague pronouns. Second,from the examples we have seen, we would argue thatthe application of heuristics could heavily improve theprecision of existing smell detection techniques. Forexample, if we exploit the information available fromPOS tagging, we can ﬁnd out whether a comparisonrefers to a number or numerical expression.Regarding recall, our analysis shows only a slightvariance between the smells, with the only outlierbeing the Superlative Requirements Smell; how-ever, since this is a very rare smell, this recall is basedon only few data points, therefore, we must considerthis result with care. When inspecting the reasonsfor false negatives, we found that optimizations couldbe made through the lemmatizer. Future research inthis direction should compare whether the accuracy oflemmatizers as reported in the ﬁeld of computationallinguistics also holds for requirements engineering ar-tifacts. Furthermore, we analyzed requirements inGerman language where lemmatization is a more dif-ﬁcult problem than in English, since the languagemakes stronger use of inﬂections (e.g. with cases orgender). Hence, smell detectors based on lemmatiza-tion for the English language might work better thanthe results indicate in our analysis.In general, the precision and recall are thereforecomparable to other approaches with related purposes(see Sect. 2). However, is it suﬃcient for an applicationof Requirements Smells in practice?First, when looking at precision, we must take intoaccount that the current state of practice consistsstill of manual work and that the cost for runningan automatic analysis is virtually zero. Nevertheless,checking a false positive ﬁnding takes eﬀort which aninspector could rather spend in reading the documentin more detail. However, as we see a high variation inthe precision over diﬀerent smells, we need to discussthese separately. Several of the smells have a preci-sion of 0.7 and higher which is considered acceptablein static code analysis [8]. For other RequirementsSmells, the precision is below 0.5. This means thatevery other ﬁnding will be a false positive. This can


be critical in the eﬀort spent in vain and annoy auser of the smell detection. Yet, we follow Menzies etal. [57] that a low precision can be still useful “Whenthere is little or no cost in checking false alarms.” Inour experience, the cost of checking a ﬁnding is oftenjust a few seconds.


Second, when looking at recall, most of the smelldetections reach a recall of more than 80%. Variouspublications, most prominently Kiyavitskaya [40] andBerry et al. [5], argue that a recall close to 100% is abasic requirement for any tool for automatic QA inRE. The core argument is that with a lower recall, re-viewers stop checking these aspects and consequentlymiss defects, and that reviewers need to check thecomplete artifact anyway. However, if taking the ex-ample of spell checkers and grammar checks, these arestill used on a daily basis, although they are far awayfrom 100% recall. Therefore, one could consequentlyalso argue that the precision is more important thanthe recall.


In any case, whether the reported precision and re-call are suﬃcient in industry needs further research inthe future. As mentioned above, it mainly depends ontwo factors: the required investment versus the gainedbeneﬁt (similar to the concept of technical debt). Forthe required investment, we argue that, based on ourexperience of analyzing the various cases presentedhere, one can quickly iterate through the detectedﬁndings with low investment. To further support thisdiscussion, the following research question analyzesthe aspect of the beneﬁts to practitioners in moredetail.


Answer to RQ 2.1. As shown in Tables 7 and 8, andas shown in Fig. 10, the precision is on average around59%, with an average recall of 82%, but both varybetween smells. We consider this reasonable for a taskthat is usually performed manually. However, this alsodepends on the relevance of ﬁndings to practitioners,which we analyze in RQ 2.2. The study also revealsimprovements for future work through the applicationof deeper NLP.


27


GGGG


G


G


G


G


0.00.20.40.60.81.0


0.00.20.40.60.81.0


Precision


Recall


SubjectiveLanguage


Ambiguous


Adverbs


and


Adjectives


Loopholes


Non−


verifiable


Terms


Superlative


Requirements


Comparative


Requirements


Negative


Words


Vague


Pronouns


Figure 10: Precision and recall of the discussed smell detection approaches.


28


Table 8: Recall of smell detection within sample of 4artifacts (16,271 words)


Smell


Findings in artifacts


Findings identiﬁed correctly


Recall


Ambiguity-related S.74640.86Superlative Requirements S.420.50Comparative Requirements S.21200.95Negative Words S.64540.84Vague Pronouns S.37340.92


Average4034.80.82Overall2001740.87


relevant in which context?To understand whether the Requirements Smellshelp detecting relevant problems, we ﬁrst performeda pre-study, in which we confronted practitioners ofDaimler and Wacker with ﬁndings. The pre-study,which we reported in Femmer et al. [24], aimed atreceiving qualitative and tacit feedback. It showedthat Requirements Smells can in fact indicate relevantdefects.In contrast, in this study we analyze relevance inspeciﬁc categories by interviewing practitioners atTechDivision on their opinion on the ﬁndings in termsof relevance, awareness, and whether these practition-ers would resolve the suggested ﬁnding.


Quantitative observations. Table 9 reports the 20 ﬁnd-ings that we discussed with TechDivision. In summary,we can see that they considered 65% of the ﬁndingsas relevant for their context. Furthermore, they havenot been aware of 45% of the ﬁndings. Lastly, theywould act on 50% of the presented ﬁndings and on40% even immediately.


Qualitative observations (true positives). The ﬁnd-ings that the tool produces mostly constituted formsof underspeciﬁcation. For example, in Finding #1 (seeTable 9): "As a searcher, I want to see the checkboxesin the diﬀerent categories displayed more clearly, sothat. . . " (for similar examples, see Findings 3, 4, 14,16, and 20). In this case, as in many of the other exam-ples, the practitioners stated that no developer couldimplement this story properly. They also recalledvarious discussions in estimation meetings on whatwas to be done to complete these types of stories13.In the previous research questions, we have seen thatRequirements Smells are able to detect loopholes inrequirements, such as the usage of the word should. Tounderstand the relevance of this ﬁnding in the contextof an agile company, we also discussed the loopholein Finding #6. When we pointed out the ﬁnding, theyresponded that they considered expressing what thesystem should do in user stories problematic. Theyconsidered this defect a low risk, as the developersunderstood ("If you are told that you should take outthe trash, you understand that it is an imperative.")and their user stories did never turn out to be of legalrelevance. They concluded that they want to avoidthis, but it has no immediate urgency in a projectsituation.ISO 29148 discusses the use of negative state-ments ("capabilities not to be provided"). In a previ-ous study [24] practitioners expressed their reluctanceof this criterion. In contrast, in this study, practition-ers said they would act upon 2 out of 3 of the negativestatements (Findings #9–11) that we presented tothem as they revealed unclear requirements. In onecase they even remembered that this led to discussionsabout the implementation during the sprint. Table 9shows many more, similar examples.


Qualitative observations (false positives). Also inter-esting are those cases that practitioners considered not


13Note that discussions can have diﬀerent objectives, i.e. whatis to be implemented and how. For these, how to implement astory is the team’s task and thus discussions can help ﬁndingthe best way. In contrast, what the product owner wants isoutside of the team’s scope and therefore should not be a matterof discussion.


29


relevant in their context or where practitioners saidthey would not act upon. Summarized, the reasonswere the following:


Domain and context knowledge: Somestoriesthat were unclear to outsiders were understand-able for someone knowing the system underconsideration. For example, in user story #18 itwas unclear to the ﬁrst and second author whattheir refers to. It was clear, however, to bothpractitioners with knowledge about the system.


Process requirement: In Finding #8, the smell re-veals another conspicuous ﬁnding: The developershould put as low eﬀort as possible into the im-plementation of this story. In the discussion, thereason for this was that the customer did notwant to pay much for this implementation. Thusthe story should only be fulﬁlled if it was possibleto be fulﬁlled cheaply. While the practitionerstold us they would not change anything aboutthis story, they agreed that the smell pointedout something that violates common user storypractice.


Finding in reason part: In four cases, the practi-tioners agreed to the ﬁnding but considered itirrelevant as the ﬁnding was inside the reasonpart of the user story. This is due to this part ofthe user story only serving as additional informa-tion. This reason part is not used in testing noris the information directly relevant for implemen-tation. The main purpose is to understand thebusiness value and to indicate the major goal tothe team, similar to goals and goal modeling intraditional requirements engineering [50].


Answer to RQ 2.2. In summary, the practitionersexpressed that 65% of the discussed ﬁndings wererelevant, as they lead to lengthy discussions and un-necessary iterations in estimation. They also saw theproblem of legal binding, but in contrast to the prac-titioners of Case A and B, they considered these ﬁnd-ings less relevant. Due to these results, they expressedtheir strong interest in exploring smell detection forprojects; we will explain the results of this discussionin RQ 4.


Further observations of quality defects in diﬀerentparts of a user storyWe considered especially the last explanation for re-jecting ﬁndings (ﬁnding in reason part of a user story)particularly interesting. We had noticed that the rea-son part was often written in a rather imprecise way.To be able to quantify this aspect, we automaticallysplit user stories according to the language patternsand quantiﬁed the distribution of words as well asﬁndings over the diﬀerent parts of user stories.Table 10 shows the results of this analysis. Thenumber of words is roughly distributed as follows: 11%of the words of a user story describe the role, 55% ofthe words describe the feature and 34% describe thereason. Of the 1,082 user stories, 290 had no reasonpart at all. Due to this uneven distribution, similar asin the previous analyses, we normalize the number ofﬁndings by the number of words in each part resultingin the number of ﬁndings per 1,000 words.Only 1% of the ﬁndings are located in the rolepart. In fact, when we inspected these ﬁndings, theywere false positives due to the grammatical problemsdescribed in the previous section. The absence ofﬁndings in this section is expected, as this part ofthe user story only names the role and does not oﬀermany chances for smells as described in Sect. 3.2. Forthe remainder, 46% of the ﬁndings are located in thefeature and 53% are located in the reason part. Inrelation to its size, the diﬀerence is striking: With 64ﬁndings per 1,000 words, the reason has nearly doublethe number of ﬁndings of the feature part and nearly70% more ﬁndings than the average requirement, asIn summary, the reason part of user stories is par-ticularly prone to smells, but the qualitative analysisin RQ 2.2 reveals that practitioners consider ﬁndingsin this section to be less relevant. This investigationcould support further application of RequirementsSmells in practice by helping to prioritize smells ac-cording to their location.


6.2.5. RQ 3: Which requirements quality defects canbe detected with smells?For 44 of the 51 requirements artifacts the stu-dents provided technical reviews. We qualitativelyanalyzed the results of 10 randomly selected reviews


30


(around 20%). The inspected reviews were conductedby 5–7 reviewers (mean: 5.6), took 90 minutes andresulted in 18–69 defects (mean: 38.1). We iteratedthrough the 381 defects documented in the reviewsand evaluated whether the smell detection producedﬁndings indicating these defects. If no smell indicatedthe defect, we openly classiﬁed the defects. We didnot quantify these results, because the resulting num-bers would assume and suggest that the distributionof defects is representative for regular projects, whichwe are unsure about (i.e. because of a high number ofspelling and grammatical issues).The classiﬁcation of the defects and their compari-son with the detected smells resulted in the followinglist of of defects indicated by Requirements Smells:


Sentence not understandable. In some instances,when the defect suggested changing the sentenceto improve understandability, these sentenceswere highlighted especially by the vague pro-nouns and negative statements smells.


Improper legal binding. Various requirements ar-tifacts had issues with improper legal binding.In one case, the reviewers recognized this anddemanded the use of the term must. The loop-holes smell pinpointed at this issue.


Unspeciﬁed/unmeasurable NFRs. Varioussmells,especiallythesuperlatives smell,indicated at defects of underspeciﬁcation withinnon-functional requirements.


The remaining defects were not indicated by Re-quirements Smells.


Interpretation. The quantitative distribution of de-fects is not necessarily representative for industryprojects and, thus, has not been not analyzed. The re-views clearly show that manual inspection discoveredthe same defects as in the previous research question:Understandability, legally binding terminology andunderspeciﬁed requirements. These are issues with re-gards to representation but also the content describedin the artifact. We argue that these issues are com-mon for requirements artifacts. Requirements Smellscan therefore indicate relevant defects from multiple,


independent sources (manual inspection, interviewswith practitioners, independent manual reviews) formultiple, independent cases.


Answer to RQ 3. Automatic smell detection can pointto issues in both representation (e.g. improper legalbinding) and content (underspeciﬁed/unmeasurableNFRs). The analysis of the reported defects indicatesthat more defects could be automatically detected(see section further discussion on detectability of de-fects described next). Nevertheless, just as for staticcode analysis, we see that automatic analysis can notindicate all defects and thus must be accompanied byreviews [73]. The fourth research question aims atanalyzing this aspect in depth.


Further discussion on detectability of defects. Duringthe analysis, if no smells indicated the defect, weopenly classiﬁed the defects. While discussing theresulting list of defects and the degree to which theyare detectable within the group of authors, we cameup with a classiﬁcation which is broader as initiallyplanned while designing the study. This classiﬁcationconsiders whether a defect:


• Already can be detected• Could be detected, but is not implemented yetin our detection


• Cannot be detected at the moment, but should besoon


• Cannot be detected at all and probably won’t besoonThis classiﬁcation is purely based on our knowledgeof existing related work and our subjective expecta-tions gained during the data analysis process. Theclassiﬁcation yielded in a map visualised in Fig. 11.The ﬁgure is structured in two dimensions: On thevertical axis, we group the defects into defects relatingto the content, and defects relating to representation.Furthermore, on the horizontal axis, we map the itemsaccording to the expected precision and completenesswe believe the detection could be (i.e. the classiﬁ-cation above). The further left an item, the moreprecise and complete we expect a smell detection tobe; the items on the right we assume to be close toimpossible to detect in a general case.


31


Detected&


by&Requirements&Smells


Sentence not understandable


Unspeciﬁed/unmeasurable NFRs


Improper legal binding


Incorrect information


Unintuitive Use Case ﬂow or diagrams


Language'seman*cs


Semantic clones


Missing mandatory items


Terminology


Presenta*on'and'Structure


Representa5onContent


Singularity in UC


EncodingUnnatural itemizationsUnintuitive structure of table


Unappealing imageUnreadable image


Underspeciﬁed termsUnnecessary terms in glossary


Naming violating convention 


Undeﬁned domain-speciﬁc terms


Inconsistent usage of terms


Incomplete information


Spelling


Grammar


Language mixture


Wrong word (language)


Semantically contradicting information


Detectable


by&Requirements&Smells


Rather&not&detectable


by&Requirements&Smells


Wrong word (domain)


Structural redundancy / Cloning


Structurally inconsistent diagrams 


Figure 11: Findings in requirements reviews, classiﬁed by content/representation and detection


32


With the defects that our current approach does notreveal, this research question shows that more defectscould be detected: These are namely defects withterminology, singularity in use cases and structuralissues focusing on the content such as the absenceof mandatory elements in the artifact [37], structuralredundancy [34] or structural inconsistency betweencontent. It remains unclear how far more enhancedlanguage analysis with more sophisticated NLP andontologies can enable to understand language. In anycase, when a defect remains subtle and vague in itsdeﬁnition, such as an unintuitive structuring or design,we only see potential for automation if a defect canbe deﬁned precisely. For problems relating to thedomain itself (e.g. incomplete information about thedomain or incorrect information with regards to thedomain), we consider it impossible to detect issuesunless formalizing the concepts of the domain.


6.2.6. RQ 4: How could smells help in the QA pro-cess?


After the interviews and analysis, we asked all in-volved practitioners whether or not they think re-quirements smell detection is a helpful support, andwhether and how they would integrate it in theircontext. We asked those questions openly and tran-scribed the answers for validation by the intervieweesand later coding. In the following, we report on theresults structured by topics. Where applicable, weprovide the verbatim answers in relation to their cases(A, B or C).


Overall Evaluation. In general,all practitionersagreed on the usefulness of the smell detection even ifconsidering diﬀerent perspectives that arise from theirprocess setting. One practitioner (Case C) reportsthat he expects one beneﬁt in using smell detection isthat it would lead to a reduction of the time spent foreﬀort estimations (in context of agile methods), as theproduct owner could beneﬁt from the smell detectionon the ﬂy and, thus, avoid misinterpretations later.


Quotes on Overall Evaluation


A.“I think that smells can help to analyze aspeciﬁcation.”


B.“The method of Requirements Smells is avaluable extension in the area of require-ments engineering and gives helpful inputconcerning the quality of speciﬁed require-ments in early development phases.”


C.“I think such a smell detection is of highvalue to make sure that our team is con-fronted with already quality assured [user]stories. This can reduce the time in our ef-fort estimations, because the product ownerwould directly notice on the ﬂy what couldlead to misinterpretations later.”


Integration into Process. When asked for how thepractitioners would integrate the smell detection intotheir process setting, we got varying answers depend-ing on the process. The practitioner relying moreon rich process models (Case B) could imagine usinga smell detection either as a support for the personwriting the requirements or as part of a more funda-mental QA method for the company. But also thepractitioner relying more on the agile methods (CaseC) could imagine using Requirements Smells as a sup-port for the person writing the requirements or incontext of analytical QA. In addition, one potentialuse is seen in context of problem management. Im-portantly, all practitioners see the full potential of asmell detection only if integrated in their existing toolchain (see also quotes on constraints and limitations).


Quotes on Integration into Process


B.“I like to compare Requirements Smells tothe “check spelling aid" known e.g. fromMicrosoft Word. So for me RequirementsSmells are intuitive and lightweight andshould be used and integrated within require-ments engineering and quality assuranceprocesses.”


C.“As a product owner, I would use a smelldetection could help in analytical QA, as itcould reveal when a problem occurs repeat-edly, either in a project or in the companyas a whole.”


33


Constraints and Limitations. One facet we considerespecially interesting when using qualitative data isthe chance to reveal further ﬁelds of improvement.We therefore concentrate now on the constraints thatwould hamper the usage of a smell detection. Onefacet we believe to be important is that practitionerswant to avoid additional eﬀort when using smell detec-tion in their context. Furthermore, the practitionerof Case A believes that the automatic smell detectionrequires a common understanding on the notion of REquality. He further indicates that the smell detectionshould explicitly take into account that some criteriacannot be met at every stage of a project.


Quotes on Constraints and Limitations


A.“First, the people who need to write thespeciﬁcation received training which givesthe required performance criteria. Second,abstraction levels must be taken into ac-count during the smell detection process,since at higher abstraction levels diﬀerentcriteria cannot be met (e.g. vague pronounsor subjective language).”


B.“As a product owner, I would use a smelldetection on the ﬂy provided that it wouldnot mean additional eﬀort [such as by hav-ing to use another tool].”


Answer to RQ 4. Our practitioners provided a generalagreement on potential beneﬁts of using smell detec-tion a quality assurance context. When asked howthey would integrate the requirements smell detection,they see possibility for both analytical and construc-tive QA, provided, however, this integration wouldnot increase the required eﬀort, e.g. by integratingthe detection into existing tool chains.


6.2.7. Evaluation of validityWe use the structure of threats to validity from [64]to discuss the evaluation of the validity of our study.


Construct validity. In our evaluation, we analyzedRequirements Smells in the terms of false positives,relevance and relation to quality defects. There arethreats that the understanding of these terms variesand, thus, the results are not repeatable. Yet, we are


conﬁdent that our validity procedures described inwe classiﬁed a subset of the ﬁndings independently,and afterwards compared (inter-rater agreement Co-hen’s kappa: 0.53) and discussed the results.Wesubsequently reclassiﬁed a diﬀerent subset of ﬁndingsagain, which lead to an inter-rater agreement (Cohen’skappa) of 0.72. For the classiﬁcation of false negatives,we reclassiﬁed one document separately, calculatingthe percentage of agreement on false positives14. Thislead to an agreement of 88%.We consider both of these substantial agreements,especially in the inherently ambiguous and complexdomain of RE. Thus, we consider this threat as suﬃ-ciently controlled.


Internal validity. A threat to the internal validity ofour results is that the experience of the students aswell as the practitioners might play a role in their rat-ings of relevance or detection of quality defects. Wemitigated this threat by choosing only practitionersfor the ratings and interviews who had several yearsof experience. The students are only in the secondyear. We cannot mitigate this threat but consider theeﬀect to be small. There might be some defects notfound by the students that could have been indicatedby a smell as well as unfound defects undetectableby smells. Hence, future studies will add to the clas-siﬁcation but are unlikely to change it substantially.Personal pride could potentially have an impact onthe answers to a RQ 2.2, if practitioners are not ableto professionally discuss their own work products. Inour cases, however, all practitioners openly acceptedthe discussions (as can be seen in their answers). Eventhough we carefully supervised this threat, we havenot found signs of personal bias in the cases involved.Finally, the students might also have been inﬂuencedby the review guidelines we provided. Yet, none ofthe investigated smells was explicitly listed in the


14We did not employ Cohen’s kappa here, since the numberof true positives (non-smell words) would strongly dominate theresult and therefore skew the inter-rater agreement. Instead,we calculated the ratio of ﬁndings which both rating teamsindependently classiﬁed as false positive to the number ofﬁndings which only one of the teams classiﬁed false positive.


34


guidelines. Instead, the guideline contained ratherhigh-level aspects such as “unambiguity”. Althoughwe consider this threat to be a minor one, it is stillpresent.


External validity. As requirements engineering is adiverse ﬁeld, the main threat to the external validityof our results is that we do not cover all domainsand ways of specifying requirements. We mitigatedthis threat to some degree by covering at least severaldiﬀerent domains and study objects, of which some arepurely textual requirements artifacts, some use cases,and some user stories. We argue that this representsa large share of today’s requirements practices.


Reliability. Our study contains several classiﬁcationsand ratings performed by people. This constitutes athreat to the reliability of our results. We are conﬁ-dent, however, that the peer debrieﬁng and memberchecking procedures helped to reduce this threat.


7. Conclusion


In this paper, we deﬁned Requirements Smells andpresented an approach to the detection of Require-ments Smells which we empirically evaluated in amulti-case study.In the following, we summarizeour conclusions, relate it to existing evidence on thedetection of natural language quality defects in re-quirements artifacts, and we discuss the impact andlimitations of our approach and its evaluation. Weclose with outlining future work.


7.1. Summary of conclusionsFirst, we proposed a light-weight approach to de-tect Requirements Smells. It is based on the naturallanguage criteria of ISO 29148 and serves to rapidlydetect Requirements Smells. We deﬁne the term Re-quirement Smell as an indicator of a quality violation,which may lead to a defect, with a concrete loca-tion and a detection mechanism, and we also givedeﬁnitions of a concrete set of smells.Second, we developed an implementation that isable to detect Requirements Smells by using part-of-speech (POS) tagging, morphological analysis anddictionaries. We found that it is possible to provide


such tool support and outlined how such a tool couldbe integrated into quality assurance.Third, in the empirical evaluation, our approachshowed to support us in automatically analysing re-quirements of the size of 250k words. Findings werepresent throughout all cases but in varying frequenciesbetween 22 and 67 ﬁndings per 1,000 words. Outliersindicated serious issues. An investigation of the de-tection precision showed an average precision around0.59 over all smells, again varying between 0.26 and0.96. The recall was on average 0.82, but also variedbetween 0.5 and 0.95. To improve the accuracy, wedescribed concrete improvement potential based onreal world, practical examples.A further analysis of reviews and practitioner’s opin-ions strengthen our conﬁdence that smells indicatequality defects in requirements.For these qualitydefects, practitioners explicitly stated the negativeimpact of discovered ﬁndings on estimation and im-plementation in projects.The study also showed,however, that while Requirements Smell detectioncan help during QA presumedly in a broad spectrumof methodologies followed (including agile ones), therelevance of Requirements Smells varies between cases.Hence, it is necessary to tailor the detection to thecontext of a project or company. We analyzed thisfactor in depth, demonstrating that the reason part ofa user story contains most ﬁndings (absolutely and rel-atively), but practitioners consider these ﬁndings lessrelevant as they argue that this part is not commonlyused in implementation or testing. This raises thequestion of the relevance of this part at all, at leastfrom a quality assurance perspective, which shouldbe investigated in future work.Our comparison with defects found in reviews fur-thermore showed that the Requirements Smell detec-tion partly overlaps with results from reviews. As aresult, we provide a map of defects in requirementsartifacts in which we give a ﬁrst indication whereRequirements Smells can provide support and wherethey cannot.Therefore, we provide empirical evidence from mul-tiple, independent sources (manual inspection, inter-views with practitioners, independent manual reviews)for multiple, independent cases, showing that Require-ments Smells can indicate relevant defects across dif-


35


ferent forms of requirements, diﬀerent domains, anddiﬀerent methodologies followed.


7.2. Relation to existing evidenceExisting approaches in the direction of automaticQA for RE are based on various quality models, in-cluding the ambiguity handbook by Berry et al. [7],the now superseeded IEEE 830 standard [32] and pro-prietary models. Yet, according to a recent literaturereview by Schneider and Berenbach [67], ISO 29148is the current standard in RE “that every require-ments engineer should be familiar with”. However,no detailed empirical studies (see Table 1) exist forthe quality violations described in ISO 29148. Whencomparing to similar, related quality violations, alsofew empirical, industrial case studies exist (see Ta-ble 2). Gleich et al. [30] and Chantree et al. [11] reportfor conceptually similar problems, a precision of thedetection between 34% and 75% (97% in a specialcase), and a recall between 2% and 86%. Krisch andHoudek [49] report a lower precision in an industrialsetting. The precision and recall for the detection ofthe smells, which we developed based on the descrip-tion in the standard, are in a similar range to theaforementioned. In summary, this work provides adetailed empirical evaluation on the quality factors ofISO 29148, including a deeper understanding of bothexisting and novel factors.We also take a ﬁrst step from the opposite perspec-tive: So far, to all our knowledge, all related workstarts from a certain quality model and goes intoautomation. Our results to RQ 3 provides a biggerpicture for understanding in how far quality defects inrequirements could be addressed through automaticanalysis in general.Our results to RQ 2.2 furthermore provides evi-dence for the claim by Gervasi and Nuseibeh [29] that“Lightweight validation can discover subtle errors inrequirements.”More precisely, our work indicatesthat automatic analysis can ﬁnd a set of relevant de-fects in requirements artifacts by providing evidencefrom multiple case studies in various domains andapproaches. The responses by practitioners to theﬁndings do, to some extent, contradict the claim byshould have 100% recall”. Practitioners responded


very positively on our ﬁrst prototype and the smellsit ﬁnds. Yet, obviously, more detailed and broaderevaluations, especially conducted independently byother researchers not involved in the development ofSmella, should follow.


7.3. Impact/ImplicationsFor practitioners, Requirements Smells provide away to ﬁnd certain issues in a requirements artifactwithout expensive review cycles. We see three mainbeneﬁts of this approach: First, the approach, justas static analysis for code, can enable project leadsto keep a basic hygiene for their requirements arti-facts. Second, the review team can avoid discussingobvious issues and focus on the important, diﬃcult,domain-speciﬁc aspects in the review itself. Third,the requirements engineers receive a tool for immedi-ate feedback, which can help them to increase theirawareness for certain quality aspects and establishcommon guidelines for requirements artifacts.Yet, the low precision for some of the smells mightcause unnecessary work checking and rejecting ﬁnd-ings from the automatic smell detection. Hence, atleast for now, it is advisable to concentrate on thehighly accurate smells.For researchers, this work sharpens the term Re-quirements Smell by providing a deﬁnition and a tax-onomy. By implementing and rating concrete smellﬁndings, we also came to the conclusion, however, thatnot all of the requirements defects from ISO/IEC/-IEEE 29148 can be clearly distinguished as Require-ments Smells. In particular, the diﬀerence betweenSubjective Language, Ambiguous Adverbs and Adjec-tives, Non-veriﬁable Terms, and Loopholes was not al-ways clear to us during our investigations (see RQ 2.1).Therefore, we, as a community, can take our smelltaxonomy as a starting point, but we also need tocritically reﬂect on some smells to further reﬁne thetaxonomy.Finally, empirical evidence in RE is, in general, dif-ﬁcult to obtain because many concepts depend onsubjectivity [55]. One issue increasing the level ofdiﬃculty in evidence-based research in RE remainsthat most requirements speciﬁcations are written innatural language. Therefore, they do not lend them-selves for automated analyses. Requirements Smell


36


detection provides us with a means to quantify theextent of certain defects in a large sample of require-ments artifacts while explicitly taking into accountthe sensitivity of ﬁndings to their context. Hence,this allows us to consider a whole new spectrum ofquestions worth studying in an empirical manner.


7.4. LimitationsWe concentrated on a ﬁrst set of concrete Require-ments Smells based on our interpretation of the some-times imprecise language criteria of ISO/IEC/IEEE29148. There are more smells, also with diﬀerent char-acteristics than the ones we proposed and analyzed.In addition, even though we diversiﬁed our study ob-jects over domains, methods and diﬀerent types ofrequirements, we cannot generalize our ﬁndings toall applicable contexts. We therefore consider thepresented results only a ﬁrst step towards the contin-uous application of Requirements Smells in softwareengineering projects.


7.5. Future workOur work focuses on Requirements Smells basedon ISO/IEC/IEEE 29148. Future work needs to clar-ify and extend this taxonomy based on related workand experience in practice. This also includes thedevelopment of other Requirements Smell detectiontechniques to increase our understanding about whichdefects can be revealed by Requirements Smells andwhich defects cannot.Second, this ﬁrst study gained ﬁrst insights into theusefulness of Requirements Smells for QA. We further-more sketched an integration of Requirements Smellsinto a QA process. Yet, a full integration and the con-sequences must be analyzed in depth. In particular,we need to understand whether smell detection as asupporting tool, similar to spell checking, as pointedout by on of our participants, enables requirementsengineers to improve their requirements artifacts.Lastly, Requirements Smells focus on the detectionof issues in requirements artifacts. They require athorough understanding of the impact of a qualitydefect, which is hence also part of the requirementssmell taxonomy. This link must be carefully evaluatedand analyzed in practice. Our preliminary works onthis topic [23, 59] provide ﬁrst ideas in that direction.


Acknowledgments


We would like to thank Elmar Juergens, MichaelKlose, Ilona Zimmer, Joerg Zimmer, Heike Frank,Jonas Eckhardt as well as the software engineeringstudents of Stuttgart University for their supportduring the case studies and feedback on earlier draftsof this paper.This work was performed within the project Q-Eﬀekt; it was partially funded by the German FederalMinistry of Education and Research (BMBF) undergrant no. 01IS15003 A-B. The authors assume respon-sibility for the content.


Bibliography

