
    


identifying concerns when specifying machine 


learning-enabled systems: a perspective-based 


approach 


 


hugo villamizar1*,   marcos kalinowski1,                      


h´elio lopes1, daniel mendez2 


1*pontifical catholic university of rio de janeiro, rio de janeiro, brazil. 


2blekinge institute of technology, karlskrona, sweden. 


 


*corresponding author(s). e-mail(s): hvillamizar@inf.puc-rio.br; 





daniel.mendez@bth.se; 


  


abstract 


engineering successful machine learning (ml)-enabled systems poses various challenges from both a theoretical and a practical side. among those challenges are how to effectively address unrealistic expectations of ml capabilities from customers, managers and even other team members, and how to connect business value to engineering and data science activities composed by interdisciplinary teams. in this paper, we present perspecml, a perspective-based approach for specifying ml-enabled systems that helps practitioners identify which attributes, including ml and non-ml components, are important to contribute to the overall system’s quality. the approach involves analyzing 59 concerns related to typical tasks that practitioners face in ml projects, grouping them into five perspectives: system objectives, user experience, infrastructure, model, and data.  together, these perspectives serve to mediate the communication between business own- ers, domain experts, designers, software and ml engineers, and data scientists. the creation of perspecml involved a series of validations conducted in differ- ent contexts: (i) in academia, (ii) with industry representatives, and (iii) in two real industrial case studies. as a result of the diverse validations and continuous improvements, perspecml stands as a promising approach, poised to positively impact the specification of ml-enabled systems, particularly helping to reveal key components that would have been otherwise missed without using perspecml. 


 


keywords: requirements engineering, machine learning-enabled systems, technology transfer, case study 


  


1 


1  introduction 


contemporary advances in machine learning (ml) and the availability  of  vast amounts of data have both given rise to the feasibility and practical relevance of incor- porating ml components into software-intensive systems. in this paper, we refer to them as ml-enabled systems. these systems have their behavior dictated by data instead of relying on explicitly defined rules. in other words, data replaces code to some extent. this shift from engineering purely conventional software systems to ones which have ml-components woven-in poses new challenges from the viewpoint of software engineering (se); for instance, challenges related to covering quality properties such as fairness and explainability [21], or challenges related to collaboration and mismatched assumptions in ml projects given the required multidisciplinary teams [31, 37]. these particularities typically demand extra effort to successfully develop ml-enabled sys- tems. it is, therefore, not surprising to us that gartner reports only 53% of ml projects to make it into production [19]. 


within se, requirements engineering (re) is, in simple terms, the discipline that 


is meant to effectively translate stakeholder needs into requirements, constraints, and other information that defines what software systems should do under which condi- tions [13]. due to the communication and collaboration-intensive nature, as well as inherent interaction with most other development processes, re can provide the very foundation to address several of the challenges of building ml-enabled systems [27]. for example, when developing ml models, we need to identify relevant and repre- sentative data, validate models, and balance model-related user expectations (e.g., accuracy versus inference time); just as in re for conventional software systems where we need to identify representative stakeholders, validate specifications with customers, and address conflicting requirements. 


this has also caught a new level of interest by the research community trying 


to better understand how re techniques can be extended and what challenges need to be solved to reliably build ml-enabled systems [12]. literature has shown that identifying quality metrics beyond accuracy, their specification, and understanding how they can be analyzed are not well-established yet in ml contexts [1, 2, 40, 45]. in fact, a recent roadmap for the future of se [8] emphasizes that existing re methods will need to be expanded to decouple ml problem and model specification from the system specification. on a more practical side, outside of bigtech companies with lots of experience, there is a focus on training more accurate ml models and their deployment, but rarely on the entire system including ml and non-ml components (e.g., how data is collected, how mistakes are dealt). this may lead to incomplete specifications of ml-enabled systems [23, 30], leaving most decisions to be made by data scientists [31, 48]. 


in order to help addressing  these  issues,  we  present  perspecml,  an  approach 


for specifying ml-enabled systems that involves analyzing 59 concerns grouped into five perspectives: system objectives, user experience, infrastructure, model, and data. together, these perspectives serve to mediate the communication between business owners, domain experts, designers, software and ml engineers, and data scientists. we created perspecml by following a technology transfer model [20], which is rec- ommended to foster successful transfer of technology from research to practice [50]. 


 


2 


throughout this process, we participated in real ml projects of a research and devel- opment (r&d) initiative [26], conducted a literature review on re for ml [45], created a catalogue with an initial set of concerns [46], and proposed a candidate solution for specifying ml-enabled systems [47]. in this paper, we iteratively evaluate and improve [46, 47] by conducting three studies in different contexts: (i) in an academic validation involving two courses on se for data science, (ii) with practitioners working with ml-enabled systems in an r&d initiative, and (iii) in two real industrial case studies conducted with a brazilian large e-commerce company. 


the iterative validations and continuous improvements result in perspecml, our 


approach for specifying ml-enabled systems, and collectively corroborated its poten- 


tial as a comprehensive tool for guiding practitioners in collaboratively designing ml-


enabled systems, enhancing their clarity, exploring trade-offs between conflicting 


requirements, uncovering overlooked requirements, and improving decision-making. 


furthermore, we found that the participants involved in the validations gradually 


improved their perception of perspecml’s ease of use, usefulness, and intended to use. 


the remainder of this paper is organized as follows. section 2 presents the back- 


ground and related work. in section 3, we detail how we conceive, evaluate and evolve perspecml. in section 4, we present perspecml and details its elements. in sections 5 and 6, we describe the evaluation in academia and with industry repre- sentatives. section 7 reports on industrial case studies. section 8 and section 9 raises potential threats to validity and discusses our research findings. by last, in section 10, 


we conclude the paper. 


 


2 background and related work 


this section introduces a background on the core essence of ml and presents partic- ularities and challenges when engineering ml-enabled systems that re may address. we also describe related work. 


 


2.1 ml in a nutshell 


ml is the study of computer algorithms that explores data to determine the best way to combine the information contained in the representation (training data) into a model that generalizes to data it has not already seen [35]. these systems, unlike non- ml, base its behavior on external data instead of explicitly programming hard rules. however, data may not be adequate and lead to bad outcomes. the output of the ml model is a prediction, sometimes surprisingly accurate and sometimes surprisingly inaccurate. when an ml model is integrated into a functional system, it becomes an ml-enabled system. this supposes a change in the way of designing, developing and testing these type of systems. 


 


2.2 quality of ml-enabled systems 


assuring the quality of ml-enabled systems is essential since these systems are increas- ingly becoming part of our daily life. however, this is not an easy task. their quality goes beyond ml model performance metrics such as accuracy, precision or recall. 


  


3 


typically, these ml model performance metrics comprises the primary goal of data scientists during ml model development. a good ml-enabled system is one in which the learning improves over time, particularly when the learning improves by getting feedback from users. this implies taking care of not only data and models, but also business context, user experience, infrastructure and integration of several services. when designing an ml-enabled system is important to understand the constraints on its operation. for example, where will the model run? what data will it have access to? how fast does it need to be? what is the business impact of a false positive? a false negative? how should the model be tuned to maximize business results? an ml model is just one component of an ml-enabled system as a whole. there is an incredible amount of work to be done between the development of an ml model, the incorpo- ration of it into a system and the eventual sustainable customer impact [6, 23, 30]. thinking about possible strategies to address these concerns increases the chance of designing and development an ml-enabled system that meets customer’s needs, and can avoid often costly problems later. 


 


2.3 re for ml-enabled systems 


requirements engineering (re) constitutes approaches to understand the problem space and specify requirements that all stakeholders agree upon. as such, it is con- centrates on understanding what the actual problem is, what needs towards a system result and how to resolve potential conflicts, and it is thus characterized by the involve- ment of interdisciplinary stakeholders and often resulting in uncertainty [49]. re is often considered a crucial and challenging stage of any software project. indeed, most of the problems in software systems with and without ml components come from poor requirements rather than faulty implementation. in this line, k¨astner [27] states 


that an ml model can be seen as a specification based on training data since data is a learned description of how the ml model shall behave. this means that the learned behavior of an ml-enabled system might be incorrect, even if the learning algorithm is implemented correctly. 


practitioners argue that the incorporation of ml implies addressing additional 


qualities, setting more ambitious goals, dealing with a high degree of iterative experi- mentation, and facing more unrealistic assumptions [36]. it is therefore reasonable to assume that handling and resolving validation problems is (or should be) in scope of the role of a requirements engineer. we further argue that investing in re can help to identify and mitigate problems early on. nevertheless, establishing re may be difficult due to the lack of guidance, tools, and techniques to support the engineering of ml- enabled systems [1, 45]. it is not surprising that ml-enabled systems are rarely built based on comprehensive specifications [31, 32] and that re is seen by practitioners as the most difficult phase in ml projects [23]. 


in the last years, the literature  on  re  for  ml  has  focused  on  issues  with 


data requirements [9], process of data-driven projects [48], challenges of addressing non-functional requirements and particularities of certain quality attributes such as explainability, transparency and fairness [11, 21, 34]. despite the important contribu- tions in the field so far, the importance of specifying ml components in a way that 


  


4 


customers can understand and analyze it to make adequate decisions is too often over- looked [15], and only a limited number of studies have looked into how to specify and document requirements for ml-enabled systems [1, 2, 40, 45]. for instance, berry [7] states that the measures used to evaluate a learned machine, the criteria for accept- able values of these measures, and the information about the ml context that inform the criteria and trade-offs in these measures, collectively constitute the requirements specification of ml-enabled systems. 


 


2.4 related work 


we subsequently highlight research that has investigated what quality attributes should be analyzed and how practitioners can specify and document requirements for ml-enabled systems. we further take a more holistic re perspective where an ml model is merely part of a larger ml-enabled system. 


dorard [16] proposed a management template for ml, also known as ml canvas, 


that can be used to describe how ml systems will turn predictions into value for end- users, considering elements such as problem definition, data collection and preparation, feature engineering, model selection, evaluation metrics, deployment, and monitoring. this is probably the most spread approach for documenting  ml-enabled  systems given its simplified representation. however, this can be seen as a limitation since ml canvas may not capture all the intricate details and complexities of real-world projects, leading to potential oversights or gaps in the analysis. we seek to bridge these gaps with perspecml by focusing on five different perspectives covering technical aspects and broader contextual concerns such as ethical considerations, legal constraints, and business implications, which can be crucial in real-world implementations. 


rahimi et al. [41] discussed on ideas for extracting and visualizing safety-critical 


requirements specifications and how a self-driving car would recognize pedestrians. the authors describe how re can be useful to better understand the domain and context of a problem and how this helps to better select a high-quality dataset for model training and evaluation. we are aware that identifying gaps in the associated dataset and the constructed ml model is essential to improve the overall quality, fairness, and long-term effectiveness of the ml-enabled system, but at the same time other external components such as those related to the operation (e.g., data streaming) play an important role and can make the difference between an ml-enabled system that fits customer’s needs and one that doesn’t. 


in an effort to model a representation of data-driven systems, several works have 


been proposed. for instance, chuprina et al. [10] presented an artefact-based re approach that encompasses four layers: context, requirements, system, and data. while the context specification captures the operational environment of a system, the require- ments specification covers the user-visible black-box behaviour and characteristics such as explainability, transparency and ethics. on the other hand, the system speci- fication defines the solution space and considers the system in a glass box view. the data-centric layer captures artifacts such as training and test datasets, and verifying algorithms. similarly, nakamichi et al. [38] proposed a requirements-driven model to determine the quality attributes of ml-enabled systems that covers perspectives such as environment/user, system/infrastructure, model, data and quality characteristics. 


  


5 


despite the important contributions of these works, we found some limitations when compared to perspecml. firstly, our intention is to be more specific, including more fine-grained attributes for each layer/perspective and modeling their relationships so that practitioners can have a complete view of the ml context and the software sys- tem as a whole. secondly, we detail ml-related concerns that we faced in practice that were not considered as part of their proposals, such as concerns related to business requirements and user experience, which in our context showed being important for the success of ml-enabled systems. 


another study we consider relevant is one conducted by nalchigar [39]. they 


reported on an empirical study that evaluates a conceptual modeling framework for ml solution development for the healthcare sector. it consists of three views consumed by business people, data scientists, and data engineers. the business view shows how business goals are refined into decision goals and question goals, and how such ques- tions can be answered by ml. the analytic design view models a solution in terms of algorithms, non-functional requirements and performance indicators. lastly, the data preparation view conceptualizes the design of data preparation tasks in terms of data tables, operations, and flows. we also find this work as relevant as the previous ones, but we believe that other views related to the operation of ml-enabled systems such as infrastructure and user experience must be considered to support the activities of practitioners such as software and ml engineers, and designers. 


more recently, siebert et al. [43] presented a formal modelling definition for qual- 


ity requirements in ml-enabled systems that allows to identify attributes and quality measures related to components such as model, data, system, infrastructure and envi- ronment. we consider this work strongly related to ours. for instance, the authors discusses quality attributes of an ml-enabled system beyond the ml components, just as perspecml proposes. it is also explicit about considering multiple perspec- tives: of the entire system, and of the environment the system is embedded it. as a key difference between the works, we provide a diagram that summarizes the per- spectives, the quality attributes/concerns, and shows their relationships. this seeks to facilitate effective communication and collaboration among stakeholders, provide a visual representation that can be easily understood by technical and non-technical team members, capture and document various aspects of the ml-enabled system’s design, and support analysis and verification activities. 


similarly, maffey et al. [33] proposed mlte, an initial framework to evaluate 


ml models and systems that provides domain-specific language that  teams,  includ- ing model developers, software engineers, system owners, can use to express model requirements, an infrastructure to define, generate, and collect ml evaluation metrics, and the means to communicate results. while mlte defines a general measurable process to evaluate ml systems, our proposal differs by going a step back and point- ing out typical concerns involved when setting objectives and defining key components of ml-enabled systems. we see mlte and perspecml as tools that can comple- ment each other by supporting practitioners from different angles, since they share the same purpose of early addressing practical problems faced by multidisciplinary teams throughout the ml development process. 


   


6 


3 methodology for conceiving perspecml 


in this section, we describe the process we followed to design and evaluate perspecml based on the technology transfer model introduced by gorschek et al. [20]. we used this model since our research method involved evaluations in both academia and industry with the aim of scaling the proposal up to practice, for which this model is recom- mended [50]. this mix of evaluations provides an opportunity to gather user feedback and incorporate it into the solution design. by involving stakeholders and practition- ers in the evaluation process, we gathered valuable insights about their experience, needs, and preferences. this feedback informed iterations and refinements of the solu- tion, making it more user-centric and aligned with actual user requirements. fig. 1 outlines the seven steps of the model, which we will describe sequentially hereafter (while following the terminology of the transfer model). 


  


fig. 1 technology transfer model proposed by gorschek et al. [20] 


   


3.1 step 1: identify improvement areas based on industry 


needs 


we followed the principle of constructivism [18] that advocates that a person needs to understand how something works before exploring the different ways to construct solu- tion proposals. during the last four years, the first author has participated in research and development (r&d) projects designing and developing ml-enabled  systems. these projects involve different types of ml tasks (e.g., supervised and unsupervised learning, computer vision) and algorithms (e.g., decision trees, logistic regression, neu- ral networks).this experience allowed us to assess current practices, observing domain and business settings, understand typical industry needs for ml-enabled systems, and issues related to their development. more specifically, we identified i) how important the domain and business settings are to align the stakeholder needs, requirements, and 


 


7 


constraints with the engineering and data science activities ii) interdisciplinary teams typically involved in ml projects, and iii) the lack of tools and documents that can capture key components when specifying ml-enabled systems. 


 


3.2 step 2: formulate a research agenda 


in order to better define the problem and gain more insights into existing solutions and what needs to be created, we conducted a systematic mapping study on re for ml [45], analyzed later literature reviews [1, 2, 40] and took advice from an industry-oriented publication based on more than a decade of experience in engineering ml-enabled systems [22]. here, we identified, for instance, i) additional quality attributes of ml- enabled systems that practitioners should analyze ii) the lack of studies focused on identifying key components of ml-enabled systems that may later be specified, and iii) the lack of studies evaluated in practice to validate its effectiveness, feasibility and gather user feedback. 


 


3.3 step 3: formulate a candidate solution 


after observing and gathering experience from real-world ml projects and reviewing the literature, we decided to focus on the creation of a candidate solution that can support the design of ml-enabled systems. as a first step, we proposed a catalog of 45 concerns to be analyzed by practitioners with the aim at identifying key compo- nents of ml-enabled systems [46]. the initial set of concerns were evaluated in a focus group with practitioners with different levels of experience of a r&d initiative, more specifically, three data scientists, two developers and three project leads. their feed- back was positive as they perceived the catalog of concern as prominent, and allowed us to identify initial improvements. fig. 2 shows the catalog. 


 


fig. 2  initial catalog of concerns [46] 


   


8 


therefrom, we used this catalog to create a candidate solution for specifying ml- 


enabled systems [47]. this candidate solution modeled the concerns in a structured manner by proposing a diagram that categorizes the concerns into perspectives, point- ing out relationships and stakeholders involved in the analysis of the concerns. the purpose was to capture essential information about the desired functionality, com- ponents, and constraints of the ml-enabled system. fig. 3 shows the diagram we proposed in a first effort to specify ml-enabled systems. 


  


fig. 3 initial diagram for specifying ml-enabled systems [46] 


  


in this paper, we iteratively improve this candidate solution by conducting three 


different evaluations that are briefly described hereafter. the resulting approach, which we baptized perspecml, is detailed in section 4. 


 


3.4 steps 4, 5, and 6: evolution and transfer preparation 


through validation 


the goal of these steps was to refine the candidate solution towards its industry- readiness. in order to accomplish this goal, we conducted three evaluations in different contexts, as suggested by the technology transfer model [20]: (i) with students from two courses on se for data science specifying an ml-enabled system for a toy sce- nario (validation in academia), (ii) with practitioners working in a r&d initiative discussing specifications of ml-enabled systems built retroactively with stakeholders of real projects (static validation), and (iii) in two industrial case studies conducted with an e-commerce company, specifying real ml-enabled systems from scratch using the approach (dynamic validation). note that, according to [20], the terminology ‘static’ refers to evaluating the candidate solution off-line, involving industry partici- pants and real artifacts, but not as part of a real project life-cycle activity, which is the ‘dynamic’ one. with these iterative validations we seek to ensure early issue detec- tion, user satisfaction, continuous improvement, adaptability and overall confidence in the final solution. details on the validations are provided in section 5, 6, and 7. 


  


9 


3.5 step 7: release the solution 


perspecml, which is presented in the next section, is now being adopted within the r&d initiative involved in the static validation to specify their ml-enabled system projects. in addition, the approach has been successfully transferred to the data science team responsible for the two case study projects involved in the dynamic validation. at first, the team decided to limit perspecml to ml projects involving supervised learning tasks. the full adoption is pending results from other evaluations. 


 


4 perspecml 


in this section we present perspecml, a perspective-based approach for specifying ml-enabled systems that involves analyzing 59 concerns related to typical tasks that practitioners face in ml projects when defining and structuring these software systems. the concerns are grouped into five perspectives: system objectives, user experience, infrastructure, model, and data, providing a structured way to analyze and address different aspects of the ml-enabled system. together, these perspectives align the activities between business owners, domain experts, designers, software and ml engi- neers, and data scientists. by using perspecml, practitioners are expected to be able to: 


• enhance clarity: different stakeholders such as software engineers and data scien- 


tists may have varying goals, requirements, and concerns. modeling perspectives and tasks helps to identify and explicitly represent these diverse viewpoints, ensuring a clear understanding of the ml-enabled system from multiple angles. 


• foster collaboration: providing a perspective-based approach encourages col- 


laboration and communication among stakeholders. it facilitates discussions and negotiations by providing a common structure to express and compare different viewpoints. 


• identify trade-offs: perspectives and concerns enable the exploration of trade-offs 


between conflicting objectives and requirements. by explicitly modeling  a  high- level ml-enabled system workflow, practitioners can analyze the impact of design decisions on each perspective and make informed choices that balance different concerns. 


• improve decision-making: understanding the tasks and concerns of both ml 


and no-ml components helps practitioners to evaluate and compare alternative solutions, enabling informed decision-making as the project progresses. ml projects are full of decisions that stakeholders must make. 


• ensure completeness: by considering multiple perspectives and concerns, prac- 


titioners can uncover hidden or overlooked requirements or risks. this helps in ensuring that the final ml-enabled system addresses the needs of all stakeholders and avoids potential pitfalls or shortcomings. 


in the following, we detail each element of perspecml that we evolved throughout 


the iterative validations we conducted. we describe the stakeholders, the perspectives and their concerns, the relationship between them, and the two final artifacts that structure the above elements: the perspective-based ml task and concern diagram 


  


10 


and the corresponding specification template. we also describe the logical flow for executing perspecml. 


 


4.1 stakeholders 


building successful ml-enabled systems requires a wide range of skills, typically by bringing together team members with different specialties [22, 28]. taking a holistic system view is essential because ml expertise alone is not sufficient and even engineer- ing skills to, for example, build pipelines and deploy ml models cover only small parts of the software system. we also need to be concerned about how to improve the expe- rience of end-users in order to deal with unrealistic assumptions, and align business value to ml technical activities in order to cover business requirements. given this, we seek perspecml to impact the work of business owners, domain experts, designers, software/ml engineers, data scientists and requirements engineers. 


business owners (bo) should understand what properties and components are 


essential to achieve the business objectives and be aware of the ml capabilities in order to set realistic goals and expectations. for instance, how to connect business objectives with ml outcomes? what is the real cost involved in maintaining an ml- enabled system? what team and skills are needed to successfully building ml-enabled systems? 


domain experts (de) play an important role in accurately defining the problem 


in a way that aligns with real-world scenarios and requirements, ensuring that the ml-enabled system addresses the specific challenges and objectives of the domain. by collaborating closely with domain experts, other stakeholders can benefit from their in-depth knowledge and insights to define relevant features and data sources, and interpreting the results of the ml model in a meaningful context. 


designers (dg) collaborate to translate complex ml concepts  and  model  out- 


puts into intuitive and easy-to-understand interfaces that provide value to end users. for instance, where and how the ml outcomes will appear? how often it will appear? and how forcefully it will appear? a good user experience must be on the user’s side and make them happy, engaged, and productive. creating interactions with users to get feedback and grow learning is essential to ensure the quality of the ml model over time. 


software/ml engineers (se) should understand how the entire system will 


interact with the ml model. they work on transforming the data scientists’ research prototypes into ml-enabled systems that can handle large-scale data, ensure scala- bility, and meet performance concerns. for instance, what are the pros and cons of deploying an ml model as a back-end application or as a web service? online or batch predictions are enough to meet user demand? 


data scientist (ds) leverages their expertise in data analysis, statistical model- 


ing, and ml algorithms to extract insights, develop ml models, and drive data-driven decision-making, but they should also understand the constraints these systems put on the ml models they produce. for instance, what quality properties the ml model should consider? what domain restrictions may apply? what should be the complex- ity of the ml model? and how should the ml model be tuned to maximize business results? 


 


11 


requirements engineers collaborate closely with stakeholders to support the 


discussions between business owners, domain experts, and data scientists, and the development team, facilitating effective communication and understanding of project requirements. we seek to empower requirements engineers by using perspecml to identify and resolve conflicts often associated with ml projects. for instance, how much loss of accuracy is acceptable to cut the inference latency in half? can data scientists sacrifice some accuracy but offer better interpretability and explainability? one of the main benefits of applying re for ml projects is to help balance these concerns. 


 


4.2 concerns 


in se, a concern typically refers to a specific aspect, interest, or issue that needs to be addressed or considered during the development and maintenance of a soft- ware system, consequently influencing its design, implementation and behavior. when designing ml-enabled systems and breaking them down into components, it is cru- cial to identify which attributes are important to contribute to the overall system’s quality. determining this requires a deep understanding of the system’s goals, stake- holders’ requirements, and the overall context in which the software will be used. in the case of ml components, the challenge is further amplified since it incorporates models that make predictions based on patterns and trends learned from data, which introduce unique considerations. all of these considerations, including ml components and deterministic (non-ml) components, become concerns for practitioners in charge of designing an ml-enabled system. 


one of the main elements of perspecml are its concerns. in total, we identified 59 


concerns including, for example, data streaming, model serving and telemetry when thinking on the operation of the ml-enabled system, and inference time, explainability and reproducibility when thinking on the development of the ml model. the concerns, that can be seen as quality attributes, came from i) own experiences of the authors of this work who have been actively participated in real ml projects, from ii) literature reviews on re for ml that have researched both academia and industry, and from iii) practitioners who iteratively evaluated the concerns and recommended new ones to be considered. in perspecml, the concerns are part of tasks that stakeholders typically face throughout the development of ml-enabled systems. 


 


4.3 related tasks modeling 


in perspecml we also focus on capturing and representing the tasks that should be performed by stakeholders to develop successful ml projects. in total, our approach outlines 28 tasks that are covered by the five perspectives. these tasks group associ- ated concerns that should be analyzed by stakeholders. with this feature, stakeholders can more easily understand and describe how tasks are performed, what concerns are involved, the relationships between concerns, and the interactions with other stake- holders. for instance, typically in ml projects, data scientists are tasked with training, validating, and deploying ml models. these tasks involve implicit concerns that are 


   


12 


not easily identified at first sight, such as inference time, learning time, model com- plexity and hyperparameters tuning. in addition, some specific tasks can benefit from involving more than one stakeholder in the analysis. for instance, to validate ml models it is necessary to generate model performance metrics, typically performed by data scientists, and analyze such metrics in collaboration with domain experts who deep understand the problem and data. 


in the early phases of developing ml-enabled systems, several key tasks should be 


performed to lay a strong foundation for the project’s success. these tasks typically involve all the stakeholders, and concern understanding the problem, setting goals, among other. table 1 details the tasks from a system objectives perspective. 


 


table 1  description of the tasks to define the system objectives 


 


task description 


understand the problem 


understand the problem domain and the real-world context in which the ml model will be deployed, and define the ml problem and the specific task to be solved 


set goals at different levels 


define the goals of the ml project at different levels in order to ensure that it meets the expectations of the stakeholders 


establish success indicators 


define measures that provide early insights on the achievement of the objec- tives 


manage expectations 


define what  the  ml  model  can  and  cannot  do. stakeholders  may  have unrealistic expectations about the ml capabilities, and providing clarity will prevent disappointment and frustration 


  


a positive user experience is crucial for the successful adoption, acceptance, and 


utilization of ml-enabled systems. it enhances user engagement, improves user satis- faction, and ultimately contributes to the overall success of the ml project. table 2 details the tasks should be done to ensure that ml-enabled systems become a valuable and integral part of users’ workflows. 


  


table 2  description of the tasks to ensure user experience 


 


task description 


establish the value of predictions 


determine that the ml model’s outputs are relevant, accurate, and impactful and how they contribute to achieving the project’s objectives 


define the interaction of predictions with users 


define how users will interact with predictions (e.g., frequency and forcefulness) in order to design user-friendly interfaces and workflows 


 


visualize predictions 


present ml model outputs in a visually understandable format. visual aids such as charts, and graphs can help users comprehend complex data and insights 


collect learning feedback from users 


offer feedback mechanisms to users in order to provide updates on ml models 


ensure the credibility of predictions 


ensure that users have a clear understanding of the ml model’s capabilities and potential inaccuracies 


    


13 


a robust and well-designed infrastructure is fundamental for the success of ml 


projects. it enables efficient development, deployment, and scaling of ml models. table 3 details the tasks of the infrastructure perspective. 


  


table 3  description of the tasks to support the infrastructure of ml-enabled systems 


 


task description 


transport data to the model involves moving the relevant data from its source to the ml model for analysis, training, or prediction 


make the ml model available 


refers to the process of deploying and exposing the trained ml model so that it can be accessed for making predictions 


update the ml model refers to the process of making improvements or modifications to an existing ml model to enhance its performance 


store ml artifacts involves the systematic storage and management of various artifacts generated throughout the ml development process 


observe the ml model involves analyzing the performance, behavior, and outcomes of both the ml model and the software system 


automate end-to-end ml workflow 


involves the design and implementation of a systematic and streamlined process that automates the ml workflow, from data preparation to model deployment and monitoring 


integrate the ml model involves incorporating the trained ml model into the larger software system where it will be used for making predictions 


evaluate the financial cost involved with infrastructure 


assess and analyze the expenses related to the computational resources, hardware, software, and services required to sup- port the ml project 


  


a structured ml model development process fosters transparency, reproducibility, 


and accountability. it supports the creation of robust, reliable, and trustworthy ml solutions. table 4 details the tasks of the model perspective. 


  


table 4  description of the tasks to support the creation of ml models 


 


task description 


 


select and configure the ml model 


shortlist a set of ml algorithms that are well-suited for the task at hand, and experiment with different combinations of hyperpa- rameters  to  find  the  optimal  configuration  that  yields  the  best performance 


train the ml model create a ml model that captures the underlying patterns in the data and can make predictions on unseen examples 


validate the ml model ensure that the trained ml model meets the desired criteria 


 


deploy the ml model 


make the trained ml model available and operational in a produc- tion environment, allowing it to serve predictions to end-users or other systems 


 


evaluate other quality characteristics 


assess various aspects of the ml model beyond its predictive accu- racy. other quality characteristics are equally important for the model’s overall performance, reliability, and suitability for real- world applications 


     


14 


the management of data in ml projects is essential for building accurate and 


reliable ml models. table 5 details the tasks to be done, mainly by data scientists and domain experts, to maintain high-quality data throughout the lifecycle of ml projects. 


 


table 5  description of the tasks to support data quality in ml projects 


 


task description 


 


access data 


involves timely obtaining and retrieving the necessary data from various sources to be used for model devel- opment and evaluation 


 


select and describe data 


involves carefully choosing the relevant data that will be used to train, validate, and test ml models, and describing the features of the data 


 


evaluate high-quality data 


involves a comprehensive assessment of the data used for training and testing ml models in order to ensure that the data meets certain criteria and standards to produce accurate and reliable results 


convert data in the representation of the ml model 


involves transforming the raw input data into a for- mat that can be processed by the ml algorithm 


split dataset involves dividing the available data into separate sub- sets for training, validation, and testing purposes 


 


define a golden dataset 


involves creating a high-quality dataset that repre- sents the problem domain and serves as the ground truth for training and evaluating ml models 


   


4.4 perspectives 


in se, a perspective refers to a representation of a system or its components. it provides a focused way of analyzing a particular aspect of the system, allowing to capture different concerns and stakeholders’ viewpoints. perspectives have been effectively used in se to model scenarios where team members work on a particular phenomena [5]. in perspecml, we modeled five perspective that are detailed as follows. 


system objectives perspective: when evaluating ml solutions,  there  is  a 


tendency to focus on improving ml metrics such as the f1-score and accuracy at the expense of ensuring business value and covering business requirements [4]. success in ml-enabled systems is hard to define with a single metric, therefore it becomes necessary to define success at different levels. this perspective involves analyzing the context and problem that ml will address to ensure that ml is targeting at the right problem; defining measurable benefits ml is expected to bring to the organization and users; what system and model goals will be evaluated; the ml expected results in terms of functionality, and ml trade-off to deal with customer expectations. table 6 details the concerns when thinking on objectives for ml-enabled systems. 


user experience perspective: a good ml-enabled system includes building 


better experiences of using ml. the goal of this perspective is to present the predic- tions of the ml model to users in a way that achieves the system objectives and gets user feedback to improve the ml model. therefore, we consider analyzing concerns such as defining what is  the added value as perceived by users  from  the predictions to their work; how strongly the system forces the user to do what the ml model 


 


15 


table 6   description of each concern of the system objectives perspective 


 


id concern addressing this concern involves specifying 


o1 context the specific circumstances, environment, or conditions in which the ml-enabled system will operate 


o2 need the requirement, desire, or gap that must be addressed to achieve a particular set of circumstances within a given context 


o3 ml functionality the nature of the learning problem and the desired outcome that the ml model is designed to achieve (e.g., classify customers) 


o4 profit hypothesis how the ml system’s outcomes will translate into tangible gains for the organization 


 


o5 organizational goals 


measurable benefits ml is expected to bring to the organization. e.g., increase the revenue in x%, increase the number of units sold in y%, number of trees saved 


o6 system goals what the system tries to achieve, with the support of an ml model, in terms of behavior or quality 


 


o7 


 


user goals 


what the users want to achieve by using ml. e.g., for recom- mendation systems this could involve helping users find content they will enjoy 


 


o8 


 


model goals 


metrics and acceptable measures the model should achieve (e.g., for classification problems this could involve accuracy   x%, pre- cision  y%, recall   z%) 


 


o9 leading indicators 


measures correlating with future success, from the business’ per- spective. this includes the users’ affective states when using the ml-enabled system (e.g., customer sentiment and engagement) 


o10 ml trade-off the balance  of  customer  expectations  (e.g.,  inference  time  vs accuracy, false positive vs false negative) 


 


indicates; how often the ml model interacts with users; how the predictions will be presented so that users get value from them; how the users will provide new data for learning; and what is the user impact of a wrong ml model prediction. table 7 details the concerns when thinking on user experience for ml-enabled systems. 


 


table 7  description of each concern of the user experience perspective 


 


id concern addressing this concern involves specifying 


u1 value the added value as perceived by users from the predictions 


u2 forcefulness how strongly the system forces the user to do what the ml model indicates they should (e.g., automatic or assisted actions) 


u3 frequency how often the system interacts with users (e.g., whenever the user asks for it or whenever the system thinks the user will respond) 


 


u4 


 


visualization 


user-friendly interfaces to showcase the ml model’s outputs and facilitate its integration into the customer’s existing systems (e.g., specifying dashboard and visualization prototypes for validation) 


 


u5 learning feedback 


what interactions the users will have with the ml-enabled system in order to provide new data for learning, or human-in-the-loop systems where ml models require human interaction 


u6 acceptance how well and how the model arrives at its decisions 


u7 accountability who is responsible for unexpected model results 


u8 cost the user impact of a wrong ml model prediction 


u9 user education & training 


the need to provide user education and training on the limitations of the ml-enabled system and how to interpret its outputs 


    


16 


infrastructure   perspective:  ml  models  produced  by  data  scientists  typically 


are turned into functional and connected software systems that demand special char- acteristics when in operation. the goal of this perspective is to cover the execution of the ml model, the monitoring of both data and model outputs, and its learning from new data. we consider analyzing concerns such as defining what streaming strategy will be used to connect data with the ml model; how the ml model will be served; the need for the ml model to continuously learn from new data to extend its knowl- edge; where the ml artifacts (e.g., experiments, ml models, datasets) will be stored; the need for monitoring the ml model and data; the strategy to automate ml opera- tions that allow to reproduce and maintain ml artifacts, and the integration the ml model will have with the rest of the system functionality. table 8 details the concerns when thinking on the infrastructure for ml-enabled systems. 


 


table 8  description of each concern of the infrastructure perspective 


 


id concern addressing this concern involves specifying 


i1 data streaming what data  streaming  strategy  will  be  used  (e.g.,  real  time  data transportation or in batches) 


i2 model serving how the ml model will be executed and consumed (e.g., client-side, back-end, cloud-based, web service end-point) 


i3 incremental learning 


the need for ml-enabled system abilities to continuously learn from new data, extending the existing model’s knowledge 


i4 storage where the ml artifacts (e.g., models, data, scripts) will be stored 


i5 monitorability the need to monitor the data and the outputs of the ml model to alert/detect when data drifts or changes 


 


i6 


 


telemetry 


what  ml-enabled  system  data  needs  to  be  collected.  telemetry involves  collecting  data  such  as  clicks  on  particular  buttons  and could involve other usage data 


i7 reproducibility the need  to  repeatedly  run  an  algorithm/ml  process  on  certain datasets/experiments and obtain the same (or similar) results 


i8 maintainability the need to modify ml-enabled systems to improve performance or adapt to a changed environment 


i9 integration the integration that the model will have with the rest of the system functionality (e.g., safety, security, privacy, fairness, legal) 


 


i10 


 


cost 


the financial cost involved in executing the inferences and with the infrastructure that could affect architectural decisions. great mod- els can be unusable due to the cost to run and maintain them 


  


model perspective: building a ml model implies not only cleaning and prepar- 


ing data for analysis, and training an algorithm to predict some phenomenon. several other aspects determine its quality. this perspective involves analyzing concerns such as defining the initial candidate of expected inputs and outcomes (of course, the set of meaningful inputs can be refined during pre-processing activities); the set of algo- rithms that could be used according to the problem to be addressed; the need to tune the hyperparameters of the algorithms; the metrics used to evaluate the ml model and measurable performance expectations that tend to degrade over time; the need for explaining and understanding reasons of the model outputs; the ability of the ml model to perform well as the size of the data and the complexity of the prob- lem increase (scalability), to deal with discrimination and negative consequences for 


  


17 


certain groups (bias & fairness), to protect sensitive data and prevents unauthorized access (security & privacy); the acceptable time to train and execute the ml model, and the complexity of the ml model in terms of size and generalization. in table 9, we provide the description of the concerns that may be relevant to select, train, tune and validate a ml model. 


 


table 9  description of each concern of the model perspective 


 


id concern addressing this concern involves specifying 


 


m1 


 


algorithm & model selection 


the  set  of  algorithms  that  could  be  used/investigated,  based on the ml problem and other concerns to be considered (e.g., constraints regarding explainability or model performance, for instance, can limit the solution options) 


 


m2 algorithm tuning 


the need to choose a set of optimal hyperparameters for a learn- ing algorithm. a hyperparameter is a parameter whose value is used to control the learning process 


 


m3 


 


input & output 


the expected inputs (features) and outcomes of the model. of course, the set of meaningful inputs can be refined/improved during pre-processing activities, such as feature selection 


m4 learning time the acceptable time to train the model 


 


m5 performance metrics 


the metrics used to evaluate the model (e.g., precision, recall, f1-score,  mean  square  error)  and  measurable  performance expectations 


m6 baseline model the optional  simple model  that acts  as a  reference. its  main function is to contextualize the results of trained models 


m7 inference time the acceptable time to execute the model and return the pre- dictions 


m8 model size the size of the model in terms of storage and its complexity (e.g., for decision trees there might be needs for pruning) 


 


m9 performance degradation 


the  awareness  of  performance  degradation.  over  time  many models’ predictive performance decreases as a given model is tested on new datasets within rapidly evolving environments 


  


m10 


  


versioning 


the versions of libraries, ensuring compatibility, and handling any conflicts or issues that may arise due to dependencies. this is important for maintaining reproducibility, portability, and ensuring that the ml model can be easily set up and executed on different systems 


 


m11 


 


interpretability & explainability 


the need to understand reasons for the model inferences. the model might need to be able to summarize the reasons for its decisions.  other  related  concerns  such  as  transparency,  may apply 


m12 scalability the need for the model to perform well as the size of the data and the complexity of the problem increases 


m13 bias & fairness the need for the model to treat different groups of people or entities 


m14 security & privacy 


the need for the model to protect sensitive data and prevents unauthorized access 


  


data perspective: data is critical to ml. poor data will result in inaccurate 


predictions. hence, ml requires high-quality input data. based on the data qual- ity model defined in the standard iso/iec 25012 [25] and our own experience, we elaborate on the data perspective. in this perspective, we considered concerns such as defining from where the data will be obtained; the strategy to select data; the 


 


18 


description of data; evaluating the inherent quality data attributes (e.g., accuracy, completeness, consistency, real usage); what data operations and modeling must be applied; the expected data distributions and how data will be split into training, val- idating and test data; the time between when data is expected and when it is readily available for use, and the need for a golden dataset approved by a domain expert. table 10 details the concerns when thinking on data for ml-enabled systems. 


 


table 10  description of each concern of the data perspective 


 


id concern addressing this concern involves specifying 


d1 source from where the data will be obtained 


d2 timeliness the time between when data is expected and when it is readily available for use 


d3 data selection the process of determining the appropriate data type and suit- able samples to collect data 


d4 data dictionary the collection of the names, definitions, and attributes for data elements and models 


d5 quantity the expected amount of data according to the type of the prob- lem and the complexity of the algorithm 


d6 accuracy the need to get correct data 


d7 completeness the need to get data containing sufficient observations of all sit- uations where the model will operate 


d8 credibility the need to get true data that is believable and understandable by users 


d9 real usage the need to get real data representing the real problem 


d10 bias the need to get data fair samples and representative distributions 


d11 consistency the need to get consistent data in a specific context 


d12 ethics the need to get data to prevent adversely impacting society (e.g., listing potential adverse impacts to be avoided) 


 


d13 


 


anonymization 


the need to anonymize or pseudonymize to protect individual identities while still maintaining the utility of the data for ml purposes 


 


d14 data operations & modeling 


what operations must be applied on the data (e.g., data clean- ing and labeling) and what is necessary to convert data in the representation of the model. 


d15 data distribution the expected data distributions and how data will be split into training, validating and test data 


 


d16 


 


golden dataset 


the need for a baseline dataset approved by a domain expert that reflects the problem. it is employed to monitor other data acquired afterwards 


   


4.5 relationship between concerns 


identifying relationships that show influence and implications between the concerns of an ml-enabled system is of paramount importance for successful project outcomes. these relationships extend across various dimensions, such as system design, risk management, and resource allocation. understanding these factors allows for optimal decision-making, alignment with ml project goals, and efficient workflow planning. 


in perspecml, we highlight these relationships to (i) help stakeholders identify 


conflicting objectives and requirements, and (ii) promote transparent communication 


  


19 


between team members, ensuring the long-term viability and impact of ml projects. for instance, if users require to know the reasons of the ml model’s decision-making then the explainability & interpretability concern arises. but this may depend on the chosen algorithm since some ml algorithms tend to be less explainable than others 


(e.g., simpler ml algorithms such as decision trees, linear regression, and logistic 


regression are often considered more explainable than complex ml algorithms such as deep neural networks, random forests, and gradient boosting models). in addition, 


complex ml models may provide high accuracy, making it necessary to strike a balance between these concerns based on the specific needs and constraints of the ml project. 


identifying these relationships is also important within the infrastructure perspec- 


tive. for instance, defining the source to access data influences the implementation or setup of a data streaming solution, which is required to transport the data to the ml model. understanding these kind of relationships helps optimize the ml workflow and streamline the project execution. on the other hand, in the system objectives perspective, the ml functionality guides the selection of appropriate ml algorithms (i.e., different tasks, such as classification or regression, require specific algorithms that are suitable for the task at hand). furthermore, it affects how the ml model’s performance is evaluated and measured (i.e., different performance metrics, such as accuracy or recall are used based on the specific task). all perspecml relationships 


can be found in our online repository1. 


 


4.6 perspective-based ml task and concern diagram 


in order to provide a holistic view of the ml-enabled system that facilitates producing a description of what will be built and delivers it for approval and requirements man- agement, we present a perspective-based ml task and concern diagram that integrates the key components discussed earlier: concerns, tasks, perspectives, and stakeholders. table 11 shows the notation we used to represent these components in the diagram. 


 


table 11  legend of the perspective-based ml task and concern diagram 


 


notation description 


 


 


the diagram contains five rounded rectan- gles that represent the perspectives. each perspective is associated with a color to facilitate its identification, and is con- nected to their tasks 


 


 


the diagram contains rectangles attached to a perspective that connect a task (at the top right) to one or more concerns (at the bottom). each task has at least one actor suggested (at the top left) related to the execution of the task and the analysis of the concerns 


 


1https://doi.org/10.5281/zenodo.7743479 


   


20 


the perspective-based ml task and concern diagram shown in fig. 4 serves as a 


visual representation of the interplay between these components and their relationships within the context of ml projects. it offers a comprehensive overview of how different perspectives shape the tasks at hand, while considering the specific concerns associated with each task. additionally, it highlights the involvement  of  various  stakeholders who contribute their expertise and insights throughout the development process. by presenting this integrated diagram, we aim to provide a clear and structured approach for understanding the complex dynamics involved in building successful ml-enabled systems. 


 


 


fig. 4 an illustration of the perspective-based ml task and concern diagram 


   


4.7 perspective-based ml specification template 


documenting and organizing requirements is crucial for ensuring a clear understanding of the desired software system functionality, facilitating communication and col- laboration, verifying and validating requirements, managing changes, and enabling knowledge transfer. it plays a vital role in successful software development and project outcomes. in order to fulfill these promises, we proposed a specification template based on the perspective-based ml task and concern diagram that provides a standard- ized format for documenting and organizing the applicable concerns of ml-enabled systems. fig. 5 presents the perspective-based ml specification template for user expe- rience and infrastructure perspectives. the complete template is available in our online repository1 


instead of starting from scratch each time, stakeholders can utilize this prede- 


fined template that already includes relevant sections, headings, and prompts, saving 


 


21 


 


 


fig. 5 perspective-based ml specification template for user experience and infrastructure perspec- tives 


 


time and effort during the specification process. this reduces redundancy, and allows 


stakeholders to focus on the specific details and concerns of the ml-enabled system. 


the perspective-based ml specification template consists of a set of predefined 


questions that guide the exploration and assessment of the concerns related to the tasks and perspectives. for example, if the concern is about the strategy to stor- age ml artifacts, the template includes a question that highlights ml artifacts such as models, data, experiment, and environments. if the concern is about the strategy 


to improve the performance of the ml  algorithm,  the  template  includes  a  ques- 


tion that highlights options such as hyper-parameter tuning. by analyzing these 


question-oriented concerns, we seek that stakeholders can ensure a comprehensive and 


systematic exploration of the concerns. 


inherently to the nature of ml projects, some types of concerns (e.g., algorithm 


& model selection (m1) and data operations & modeling (d14)) are uncertain at the beginning of the project, mainly due to a common need of experimentation to get a better understanding on achievable requirements. hence, they may be refined as the project progresses. the perspective-based ml specification template we proposed, highlights these concerns with the letter “e”. 


 


4.8 perspecml’ logical flow 


in order to provide clarity, structure, reproducibility, and consistency, this section shows the steps to be followed for executing perspecml. the purpose is to break down the overall process into manageable and sequential tasks, making it easier for stakeholders to understand and follow. fig. 6 summarizes the logical flow to ensure that perspecml is executed in a systematic and organized manner, leading to more successful outcomes. 


  


22 


as a bank manager i want to automatically classify customers so that i can decide upon granting a requested loan 


 


 


fig. 6 logical flow for executing perspecml 


 


we expect perspecml to be used by requirements engineers, in collaboration with 


other stakeholders, to support the specification of ml-enabled systems. the process begins by considering the perspectives. we established an intuitive order to analyze them: system objectives, user experience, infrastructure, model, and data. given a perspective, a requirement engineer or a stakeholder performing that function can analyze each concern with the recommended stakeholders, also considering the rela- tionships between concerns. if the concern is applicable, it should be specified in the perspective-based ml specification template and classify its relevance into desirable, important or essential. 


 


5 validation in academia 


as we mentioned before, perspecml is the result of a series of validations that were conducted in different contexts. the first validation was carried out within an academic environment where students had to use the candidate solution introduced in section 3.3 to specify a toy problem. the simplified nature of the toy problem allowed for a clear understanding of how the candidate solution performed and how it could be improved. this led to valuable lessons and discoveries that were applied in the next validation with a more complex problem. in the following, we detail the validation in academia. 


 


5.1 context 


the academic validation took place in the context of two courses on se for data science with professionals, who are also students, from a brazilian logistic company called loggi2, and computer science graduate students from the pontifical catholic university of rio de janeiro (puc-rio). participants were asked to specify a feature for an ml-enabled system using the example of a bank loan problem, by analyzing the candidate solution’s perspectives and concerns. the feature consisted of automatically classifying customers into good or bad payers and was described in user story format. 


 


from the user story, we can infer that the ml component needs to access, for learn- 


ing purposes, data on customer characteristics, previously granted loans, and payment 


 


2https://www.loggi.com 


  


23 


records. regarding non-ml components and integration with other services, the par- ticipants could assume restrictions and requirements of the software system that the ml component would use. with this information, we asked the participants to ana- lyze each concern of the candidate solution and provide a reasonable specification, if applicable. thereafter they were asked to individually answer a follow-up question- naire critically assessing the relevance and completeness of the candidate solution’s perspectives and concerns. all the material provided to the participants is available in our online repository1. fig. 7 illustrates the academic validation. 


 


 


 


fig. 7 process diagram for the academic validation 


   


5.2 goal and method 


we detail the goal of the validation in academia in table 12. we followed the goal- question-metric (gqm) goal definition template [5], which is a structured approach commonly used in se and other disciplines, to help establish a clear connection between the overall goal, the specific questions that need to be answered, and the metrics used to measure progress. 


 


table 12  study goal definition of academic validation 


 


analyze the candidate solution’s perspectives and concerns 


for the purpose of characterization 


with respect to perceived relevance and completeness, and ease of use, use- fulness and intended use 


from the viewpoint of professionals and computer science graduate students 


in the context of 


two courses  with  53  data  science  professionals  from  loggi and 15 computer science students from puc-rio who were learning se for data science 


  


based on the goal, we established the following research questions for the validation 


in academia: 


• rq1: what is the relevance of each perspective of the candidate solution? we 


wanted to identify whether the perspectives of the candidate solution were perceived as meaningful and pertinent by the participants. this feedback helped confirm that 


 


24 


the perspectives align with the needs and expectations of the intended users, and allowed us to identify areas that may need refinement. 


• rq2: are the perspectives of the candidate solution and their concerns complete? 


this research question relates to the coverage of both the perspectives and concerns. this feedback helped to determine if critical components were missing or if there are gaps that need to be addressed. 


• rq3: to what extent does participants perceive the candidate solution as use- 


ful and beneficial? with this, we seek to understand the factors that influence the acceptance and adoption of the candidate solution. the question followed the tech- nology acceptance model (tam) [14] and aimed to capture participants’ overall assessment and intention to use the candidate solution, incorporating elements of perceived usefulness, perceived ease of use, and intended use. 


• rq4: what are the limitations and opportunities for improvement of the candidate 


solution? this research question seeks feedback on the approach itself. 


 


5.3 selection of subjects 


the subjects were the attendants of two se for data science courses. the in-company course at loggi had 53 professionals with different background being trained in se practices  for  building  ml-enabled  systems.  the  graduate  course  at  puc-rio  had 15 students (nine master and six ph.d students). while students may have limited expertise compared to professionals in the field, they can provide fresh perspectives, helping us identify potential blind spots. in fact, using students as subjects remains a valid simplification of real-life settings needed in laboratory contexts [17]. in table 13, we characterized the subjects by their educational background and average year of experience in ml projects. 


 


table 13  subjects involved in the validation in academia 


 


course total background experience 


(average in years) 


in-company 33 20 


computer science other discipline 


1.2 1.9 


university 15 computer science 1.3 


  


we can see that in the in-company course, not controlled by us, the professionals 


interested in data-driven projects are divided into those with a computer science back- ground and those with background in other areas such as economics and mathematics. however, it is not surprising since the literature has already noted these findings for this role [28]. overall, the participants were perceived as relatively inexperienced, as they possess only a few years of practical experience in developing ml-enabled sys- tems. while the participants were selected by convenience (attendants of the courses), we believe that their profiles were suitable for our intended initial validation. 


     


25 


5.4 data collection and analysis procedures 


to address the research questions related to the relevance, completeness, perceived usefulness, and potential improvements of the candidate solution in specifying ml- enabled systems, a questionnaire-based evaluation method was employed. this section outlines the data collection and analysis procedures used in the validation in academia. 


questionnaire design: a  follow-up  questionnaire  was  designed  to  gather 


responses from participants regarding the research questions. the questionnaire 


included a combination of closed-ended questions related to rq1, rq2 and rq3, and one open-ended question related to rq4 to get both quantitative and qualitative data. 


data collection: the questionnaire was delivered  to  the  participants  in  online 


format for the in-company course and in-person session for the university course. participants were provided with clear instructions on how to perform the specification task and how to complete the questionnaire and any specific considerations to keep in 


mind while responding. 


quantitative data analysis: for rq1, rq2, and rq3, which involve assessing 


relevance, completeness, and perceived usefulness, quantitative data analysis tech- niques were employed. closed-ended questions were used to capture participants’ ratings on  a  two-point  likert  scale  for  rq1  and  rq2,  and  four-point  likert  scale for rq3. statistical analysis, such as mean and frequency distribution were computed to summarize the quantitative data. 


qualitative data analysis:  for  rq4,  which  seeks  to  identify  potential  changes 


or additions to support practitioners, qualitative data analysis techniques  were utilized. open-ended questions allowed participants to provide detailed and descrip- tive responses. qualitative analysis involved thematic coding, categorization, and identification of patterns or recurring themes across the responses. 


interpretation and findings: the analysis of the collected data was interpreted 


according to the research questions. the findings were presented in a clear and con- cise manner, addressing each research question separately. in this case, charts were used to illustrate the results, providing a comprehensive overview of the validation in academia. 


 


5.5 results 


5.5.1 rq1. what is the relevance of each perspective of the 


candidate solution? 


this question was designed as a single choice question. to assess the relevance of each perspective of the candidate solution, participants were asked to rate the importance high or low. the perspectives considered in this evaluation included ml objectives, user experience, infrastructure, model, and data. the results indicated that all per- spectives were deemed relevant by the participants. out of a total of 68 participants, 67 considered the data perspective highly relevant, indicating its significant impor- tance in specifying ml-enabled systems. the ml objectives, model and infrastructure perspectives followed closely, at 66,65 and 63 respectively. the user experience perspec- tive received a slightly lower number of 58, indicating its relatively high but somewhat 


  


26 


“there should be a monitoring concern related to the model view. in the same way we have to train the model, we have to monitor the model outputs” 


“parameter tuning in algorithms helps improve model performance. i would include this concern” 


lesser relevance. fig. 8 presents the relevance of the candidate solution’ perspectives based on their respective ratings. 


  


fig. 8 frequencies of the relevance of each perspective of the candidate solution 


  


somehow we expect these results, since typically the main focus of practitioners 


in ml projects is data and models. in contrast, user experience concerns take a back seat to the development of ml-enabled systems. that is why with this work we seek to reinforce the importance of considering a user experience perspective. 


 


5.5.2 rq2. are the perspectives of the candidate solution and 


their concerns complete? 


this question was also designed as a single choice question with the option to explain the answer.to assess the completeness of perspectives and their associated concerns of the candidate solution, participants were provided with a list of predefined concerns corresponding to each perspective. they were then asked to indicate whether they believed the list was complete or if there were additional concerns that should be con- sidered. the results revealed that participants generally considered the initial concerns and perspectives to be comprehensive but suggested some additional concerns. only six out of 68 participants felt that something was missing. across perspectives, the model perspective had the highest number of additional concerns identified by partici- pants, highlighting the importance of monitoring ml models, optimizing parameters of ml algorithms, and breaking concepts about explainability. below are the comments of the participants in that direction. 


 


 


   


27 


 


 


5.5.3 rq3. to what extent does participants perceive the 


candidate solution as useful and beneficial? 


to gauge participants’ perception of the acceptance of the candidate solution for spec- ifying ml-enabled systems, participants were asked to rate the solution on various aspects. these aspects included ease of use, usefulness and intended use. ratings were provided on a scale of 1 to 4 (four-point likert scale), with 1 indicating strongly dis- agree, 2 indicating partially disagree, 3 indicating partially agree, and 4 indicating strongly agree. the tam questionnaire results are shown in fig. 9. 


 


fig. 9  frequencies of the tam constructs for academic validation 


 


the responses indicated a positive perception of the candidate solution. partic- 


ipants from both courses rated the solution highly in terms of usefulness, with an average rating of 3.7, suggesting that the candidate solution can support the specifi- cation of ml-enabled systems. the ease of use of the candidate solution received an average rating of 3.1, indicating that the candidate solution did not provide enough guidance to be considered clear. the intended use of the candidate solution was rated at an average of 3.3, reflecting its feasibility and applicability. overall, the candidate solution was perceived as highly useful, but showed potential for improvement in terms of ease of use. we understood that improving the candidate solution’ guidance will imply an improvement in the perception of intended use. 


 


5.5.4 rq4. what are the limitations and opportunities for 


improvement of the candidate solution? 


here, participants had the option to respond in open text format. to identify potential improvements in supporting practitioners in specifying ml-enabled systems, par- ticipants were asked to provide suggestions regarding components, perspectives, or concerns that could be changed or added to enhance the candidate solution.the analy- sis of participants’ responses revealed several valuable suggestions. as identified in the 


 


28 


“explainability could be divided into two: explainability and interpretability, given that there are explainable models that are not necessarily interpretable” 


“it would be interesting to connect more concerns because i clearly see some rela- tionships. for example, in the model perspective the explainability concern depends, to some extent, on the selection of the algorithm” 


“i would suggest explaining better how to use the approach because sometimes i did not know where to start and when to end” 


“definitely a practical example would help to better understand the proposal” 


1. in  the  infrastructure  perspective,  we  decided  to  include  ‘monitorability’  as 


a new concern, since this may require implementing different services such as real-time logging, alerts, and data drift detection 


2. in the model perspective, we broke the explainability concern into 


‘explainability and interpretability’, since these terms can have different interpretations 


3. we added ‘algorithm parameter tuning’ as a new concern of the model 


perspective, since data scientists typically need to analyze strategies to improve ml metrics 


4. we defined a set of steps to be followed by stakeholders in order to apply the 


candidate solution 


results of rq3, some participants emphasized the need to further integrate the relation- ship between concerns. others highlighted the importance of incorporating a road-map to apply the candidate solution. additionally, one participant recommended provid- ing more practical examples and case studies to enhance the solution’s applicability. in the following, we present the comments of the participants in that direction. 


 


 


 


these results provided insights into the relevance of the perspectives, the complete- 


ness of the concerns, the perceived usefulness, and potential improvements, guiding the refinement of the candidate solution. the validation in academia resulted in the following improvement opportunities. 


 


 


6 static validation in industry 


at this point, we made some improvements to the candidate solution, resulting in a version called perspecml v1. building upon the foundation of the candidate solu- tion, perspecml v1 incorporates refinements and additions based on valuable feedback and insights from the students involved in the academic validation. in this section, we detail the second evaluation that was carried out in industry where practitioners had to use perspecml v1 to retroactively specify two ready-made ml projects. we called this evaluation as static since it was performed without executing perspecml v1 in a real or simulated environment. 


 


6.1 context 


the  static  validation  in  industry   involved   practitioners   of   a   r&d   initiative called exacta3 who developed two ml-enabled system projects from different 


 


3https://exacta.inf.puc-rio.br 


 


29 


domains for a large brazilian oil company. the projects were developed following the lean r&d approach [26] and are already deployed in production in several oil refiner- ies. we refer to these projects as project a and b, since for reasons of confidentiality and undergoing patent requests they cannot be explicitly mentioned. table 14 details these projects. 


 


table 14  projects involved in the static validation 


 


project ml domain description 


a logistic regression it alerts oil refineries about the likelihood of emitting strong odors that may result in claims from the community 


 


b 


 


computer vision 


it monitors images of the flame of oil refineries, helping refiner- ies  to  decrease  the  disproportionate  burning  of  gases  that causes unnecessary energy consumption 


  


we retroactively specified project a and b using perspecml v1 with the support 


of the product owner of each project, analyzing the perspectives and their concerns, and filling a drafted specification template. this means that the specifications were added after the project had already finished. thereafter, we discussed the resulting specifications in a focus group with the practitioners who developed these projects. lastly, we provided to practitioners with a follow-up questionnaire to critically eval- uate perspecml v1. all mentioned artifacts are available in our online repository1. fig. 10 shows the process diagram for the static validation in industry. 


 


fig. 10 process diagram for the static validation in industry 


   


6.2 goal and method 


we detail the goal of the static validation in table 15. we followed the gqm template to describe what we evaluated in this first industrial validation. here, we also describe the research questions. 


in contrast with the academic validation, involving practitioners with more experi- 


ence ensures the evaluation reflects real-world scenarios and challenges. their expertise can provide valuable insights  into  the  practical  applicability  of  perspecml  v1  and its alignment with industry standards and best practices. based on the goal, we established the following research questions for the static validation in industry. 


  


30 


table 15  study goal definition of the static validation 


 


analyze perspecml v1  (academically  validated  improved  version) and its resulting specifications 


for the purpose of characterization 


with respect to perceived industrial  relevance,  ease  of  use,  usefulness  and intended use 


from the viewpoint of practitioners 


in the context of 


retroactively elaborated  ml-enabled  systems  specifications using perspecml v1 with six experienced software practi- tioners involved in the development of these systems 


 


• rq1: what problems do participants face in practice when specifying ml-enabled 


systems? we wanted to identify the challenges and difficulties encountered by partic- ipants when specifying ml-enabled systems. by understanding these problems, we analyzed the adherence to our solution, and identified the suitability of perspecml v1 to cover the needs of practitioners. 


• rq2: what perception do the participants have of the retroactive specifications of 


projects a and b derived from perspecml v1 ? by answering this research question, we gathered insights about the benefits or detriments of using perspecml v1. 


• rq3: what are the limitations and opportunities for improvement of perspecml 


v1 ? with the feedback received, we refined and enhanced perspecml v1 


• rq4: to what extent do the participants perceive perspecml v1 as easy to use, 


useful and usable in the future? through the tam questionnaire, we explored the level of satisfaction and confidence participants had in perspecml v1 as an approach for specifying ml-enabled systems. 


 


6.3 selection of subjects 


we invited six practitioners who have been actively working with the development of ml-enabled systems in the exacta initiative. before starting the focus group and providing the questionnaire, we carefully selected the participants by asking them about the role and their experience in years working with ml projects. table 16 shows an overview of the participant characterization. 


 


table 16 subjects involved in the static validation in industry 


 


id role project experience 


(years) 


p1 p2 p3 


data scientist 


a b b 


6 2 2 


p4 p5 developer a b 


2 3 


p6 project lead a 2 


  


it is possible to observe that in this study participants represent three different 


roles: data scientists who are interested in how the approach can help to build suitable and functional ml models, developers who are interested in how the approach can help 


 


31 


to design the integration between components, and project leaders who are interested in how the approach can help the team achieve its goals. this allowed to gather feedback from people who have different needs and priorities. on the other hand, participants showed have more than two years of experience, helping us determine whether perspecml v1 would work well in practice and what could be improved. note that we selected three practitioners of each project involved in the evaluation. 


 


6.4 data collection and analysis procedures 


to address the research questions, a combination of focus group discussions and ques- tionnaires were employed for data collection. in the following, we outline the data collection and analysis procedures used in the static validation in industry. 


 


6.4.1 focus group 


we conducted a focus group for promoting in-depth discussion on rq1 and rq2 [29]. focus group is a qualitative research method that involves gathering a group of people together to discuss a particular topic, allowing for interaction between the participants, which can help to surface different viewpoints. 


procedure: the focus group was conducted in a structured and moderated for- 


mat. the discussions were guided by the first author using open-ended questions related to rq1 and rq2, allowing participants to share their experiences, perspectives, and challenges faced when specifying ml-enabled systems. 


data  collection: we recorded the focus group with the consent of the partic- 


ipants to gather qualitative data. transcripts of the focus group discussions were generated by the first author from the recordings, capturing participants’ insights, ideas, and suggestions regarding rq1 and rq2. 


data analysis: thematic analysis was employed to identify common themes, pat- 


terns, and recurring topics in the focus group data [42]. the transcripts were coded, and emerging themes were categorized with the consensus of the authors. by last, the final set of categories were analyzed to address the research questions. the tran- scriptions and all codes are available in our online repository1. examples of codes are highlighted when presenting the results. 


 


6.4.2 questionnaire 


questionnaire design: the questionnaire included structured questions and rating scales designed to capture quantitative and qualitative data related to rq3 and rq4, respectively. it addressed perceptions and feedback regarding the problems faced, use- fulness of perspecml v1, ease of use, and identified limitations or opportunities for improvement. 


data collection: the questionnaire responses were collected  electronically 


through an online survey platform, taking care of anonymity and confidentiality. we provided the participants with clear definitions of the quality characteristics that we wanted to measure, ensuring that the participants understood what was asked of them. 


data analysis:  quantitative  data  analysis  techniques,  such  as  descriptive  statis- 


tics and inferential analysis, were used to analyze the questionnaire responses related 


 


32 


“to the best of my knowledge there are no tools or approaches spread in industry helping practitioners to elicit, specify and validate requirements for ml systems” 


“i’m curious to see a formal specification of an ml component. based on my experience, these definitions are informal and emerge as the project progresses” 


“sometimes i feel that the ml development team often transmits skepticism to customers, not because of the lack of knowledge of its members, but because of the lack of an established process to define what can be done in ml terms with what the customer makes available (e.g., data, business information)” 


“typically domain experts are busy, so they tend to be less involved in the early phases of ml projects. in the end, they often find unexpected results. their involve- ment is important in areas such as feature engineering, data pre-processing and model evaluation” 


“most of the time, customers expect that ml systems can solve all problems. they also don’t imagine the number of components that are required to operate and maintain an ml model over time. requirements engineering could help to address these challenges” 


to rq4. these findings provided numerical insights and trends, allowing for a compre- hensive understanding of participants’ perceptions about the acceptance of perspecml v1. qualitative data analysis techniques were also used to respond rq3, involving coding and categorization. 


 


6.5 results 


6.5.1 rq1. what problems do participants face in practice when 


specifying ml-enabled systems? 


we asked the participants about the problems they face when specifying ml-enabled systems. we coded and categorized the transcriptions of such discussions and then analyzed them to answer this research question. we found that participants frequently mentioned lack of approaches to support the specification given that ml incorporates additional challenges, which can make it difficult to specify ml-enabled systems. for instance, p6 stressed: 


 


in the same line, p4 and p5 complemented: 


 


 


on the other hand, we identified expressions about specification problems derived 


from the need to involve domain experts. for instance, p1 reported that understanding the specific domain plays a major role for accurate specifications: 


 


p4 highlighted that customers  often  overestimate  what  ml  can  do.  this  leads 


to unrealistic expectations of ml capabilities, posing challenges in the specification process. the participant expressed: 


 


these findings reflect some of the problems faced by participants in practice when 


specifying ml-enabled systems, as identified through the focus group discussions with 


 


33 


“looking at the diagram and its corresponding specifications allowed me to get an early overview of the requirements that can be refined as the project progresses. it is like a high-level guided development” 


“i found that the specifications facilitated a better understanding of the systems’ functionality, components, and data requirements, specially for project a, in which i was involved” 


“i really liked the focus on diverse aspects such as data, model, and infrastructure. this landscape facilitates the understanding of the projects” 


“identifying the tasks and concerns and their relationships allows identifying dependencies and influences as intended” 


“typically, user experience concerns are put in the background. with perspecml was possible to early specify forcefulness, a concern analyzed late in the validation phase of project b” 


“in my opinion, it is easy to convey the specifications to stakeholders, enabling bet- ter collaboration and alignment throughout the development process. for example, as a developer i can identify tasks where i need to collaborate with data scientists” 


experienced practitioners. the insights gained from these discussions shed light on the key areas that require attention to overcome challenges such as the lack of approaches to support the specification, the need to involve domain experts, and the customer unrealistic expectations of ml capabilities 


 


6.5.2 rq2. what perception do the participants have of the 


retroactive specifications of projects a and b derived from perspecml v1 ? 


after the participants analyzed the resulting specifications for project a and b derived from perspecml v1, we asked them what they thought about it. their feedback indicated positive perceptions of the specifications and their future impact on the development process. for instance, the participants highlighted that the specifica- tions acted as a guide during the development process, helping to improve the overall development workflow. p1 manifested: 


 


p1, p3 and p6 expressed that the retroactive specifications enhanced clarity and 


understanding of the ml-enabled systems for both projects: 


 


 


 


in addition, p3 mentioned that using perspecml v1 allowed to identify hidden 


concerns that are not easily identified at first sight: 


 


finally, p5 noted that the retroactive specifications derived from perspecml v1 


helped in documenting and communicating the ml-enabled systems for both projects: 


 


overall, there was a clear consensus on the benefits of the retroactive specifica- 


tions of project a and b, derived from perspecml v1. according to the participants, the specifications enhanced clarity and understanding, improved documentation and 


 


34 


“it is not clear to me how to get the specifications from analyzing the diagram. even with the provided steps to apply the solution, it is not clear to me” 


“providing tutorials or additional documentation could improve its application” 


“in my opinion, the specification template, which summarizes what the system should do, should be cleaner. i mean, the relationships between concerns are not needed as they exist in the diagram” 


“better visualizations and intuitive navigation could further enhance the user experience and ease of use” 


“in the ml objective perspective there is something that does not make sense. the ‘define objectives’ task has independent concerns that could be part of separate tasks” 


communication, acted as guide during the development process, and identified hidden concerns. 


 


6.5.3 rq3. what are the limitations and opportunities for 


improvement of perspecml v1 ? 


participants’ feedback revealed several limitations and opportunities for improve- ment. these insights, derived from the open-ended question of the questionnaire, can be related to the findings of rq4, where we had participants who expressed par- tial agreement and disagreement about ease of use, usefulness, and intended use. for instance, p1 and p2 suggested that providing additional guidance could help users grasp perspecml v1 more easily. 


 


 


participants also provided feedback on improving the user interface of perspecml 


v1, suggesting a more user-friendly design. 


 


 


on the other hand, p6 commented on improving the relationship between tasks and 


concerns. more specifically, the participant suggested breaking down a task of the ml objective perspective, since the concerns were not related at all. 


 


we identified limitations and opportunities for improvement of perspecml v1 


related to providing additional guidance, improving the user interface, and improving the relationship between tasks and concerns. some of them may be related with the participants’ perceptions explored in rq4. we addressed these limitations and cap- italized on the opportunities for improvement, allowing to refine perspecml v1 to better meet the needs and challenges identified by practitioners. 


 


6.5.4 rq4. to what extent do the participants perceive perspecml 


v1 as easy to use, useful and usable in the future? 


the participants’ responses to a tam questionnaire indicated varying degrees of agree- ment or disagreement with statements about ease of use, usefulness, and intended use. while the majority of participants totally agreed with the statements, there were a 


  


35 


5. we added the domain expert role to the perspecml  v1 ’  stakeholders, 


including it in tasks 


6. the steps defined in the academic validation to apply perspecml v1 turned 


into a workflow diagram to facilitate its application 


7. we improved the perspecml v1 documentation by creating a miro boarda 


that summarizes the perspectives, tasks and concerns to be analyzed. we also added a practical use case and explanations of each perspecml component 


8. we improved the user interface of both diagram and specification template by 


adding colors that identify each perspective and their concerns 


9. we simplified the specification template by removing the representation of 


the relationships between concerns (leaving them only in the perspective- based ml task and concern diagram, as they are used during the analysis) 


10. we checked terminology and the relationship between tasks and  con- 


cerns of each perspective to ensure its suitability 


 


ahttps://miro.com/miroverse/perspecml-machine-learning/ 


few participants who expressed partial agreement or disagreement. more specifically, one participant encountered some difficulties in using perspecml v1, two participants had reservations about its usefulness, and one participant was not fully confident in using it in the future. the tam questionnaire results are shown in fig. 11. 


  


 


fig. 11 frequencies of the tam constructs for static validation industry 


  


these varied perceptions explained to some extent the feedback received in rq3 


for identifying areas of improvement and addressing any concerns or challenges raised by participants. at the end of this validation, we decided to consider the feedback of the practitioners of the exacta initiative. in the following, we outline what was incorporated into perspecml v1 from this static validation in industry. 


 


   


36 


7 dynamic validation in industry 


based on  the  valuable  feedback  and  insights  from  the  practitioners  involved  in the static validation, we made  significant  improvements  to  perspecml  v1,  result- ing in a more robust and enhanced version called perspecml v2. in this section, we evaluated perspecml v2 by performing (i) requirement workshop sessions and (ii) interviews with practitioners who work for a large brazilian e-commerce company known as americanas that offers technology, logistics, and consumer financing services. we called this validation as dynamic, since it was performed by executing perspecml v2 for specifying two real ml projects from scratch. 


 


7.1 context 


we conducted the dynamic validation on two distinct case studies at americanas, where each case study involved a real ml-enabled system that was specified from scratch using perspecml v2. each system was assigned a team made up of novice and experienced practitioners. the purpose of these ml-enabled systems is to enhance user experience, increasing engagement, and driving business goals of the americanas company. table 17 details the ml-enabled systems that were part of this evaluation. 


 


table 17  ml-enabled systems involved in the dynamic validation 


 


system ml domain description 


 


product classification 


 


natural language processing 


it classifies titles of products registered by sellers in the marketplace of the americanas company into categories. based on the correct category, basic attributes for regis- tering the product details are then provided to the seller 


 


market recommendation system 


it suggests products to customers that are likely to be of interest or relevance to them. based on historical data and similarity measures, the products are recommended 


  


regarding the operation of these studies, we assisted practitioners in the applica- 


tion of perspecml v2 in requirements workshop sessions by providing the necessary materials and information in advance. this included documentation on perspecml v2 and example use cases. during the sessions, the practitioners analyzed and specified the ml-enabled systems by using perspecml v2. the specifications were made by adding post-its into the interactive miro board we created from the static validation. thereafter, we interviewed, in two additional sessions, the experienced practitioners who have knowledge of the domain problem and led the design and implementation of both ml-enabled systems to discuss the resulting specifications.finally, we pro- vided to all practitioners, a follow-up questionnaire to critically evaluate perspecml v2 and the resulting specifications. all mentioned artifacts are available in our online repository1. fig. 12 shows the process diagram for the dynamic validation in industry. 


     


37 


 


 


fig. 12 process diagram for the dynamic validation in industry 


 


7.2 goal and method 


we detail the goal of the case studies of the dynamic validation in table 18. we followed the gqm template to describe what we evaluated in this second industrial validation. here, we also describe the research questions. 


 


table 18  study goal definition of the dynamic validation 


 


analyze perspecml v2 (statically validated improved version) and its resulting specifications 


for the purpose of characterization 


with respect to 


the perceived quality of the specifications derived from per- specml v2, and ease  of  use,  usefulness  and  intended  use of perspecml v2 


from the viewpoint of practitioners 


  


in the context of 


two requirements workshop sessions involving 11 novice prac- titioners and three experienced practitioners who used per- specml v2  to specify two ml projects from scratch, and (ii) two interviews with the three experienced practitioners who evaluated the resulting specifications derived from per- specml v2 


  


based on the presented goal, aligned to the purpose of a dynamic industrial vali- 


dation, we defined the following research question to better understand the practical suitability of using perspecml v2. 


• rq1:  what perception do practitioners have while specifying ml-enabled systems 


by using perspecml v2 ? for this research question, we conducted a comprehen- sive evaluation of practitioners’ experiences while specifying ml-enabled systems using perspecml v2. during the requirements workshop sessions,  we  observed their interactions with perspecml v2, noted any challenges or difficulties they encountered, and gathered their feedback through discussions and direct feedback. 


• rq2: what perception do experienced practitioners have of the resulting specifi- 


cations derived from perspecml v2 ? to answer this question, we interviewed three experienced practitioners who reviewed and discussed the specifications derived from perspecml v2. we selected them since experienced practitioners can better assess the efficiency and effectiveness of perspecml v2 than novice, for instance, by comparing it to existing methods they have used in the past. during the interview, 


  


38 


the experienced practitioners provided their feedback and insights of the specifica- tions. the goal was to gather valuable insights into how the experienced practitioners perceived the quality, completeness, and suitability of the specifications produced by using perspecml v2. 


• rq3: what are the limitations and opportunities for improvement of perspecml 


v2 ? to explore this research question, we considered the feedback and discussions from both the novice and experienced practitioners. the novice practitioners’ first- hand experience with using perspecml v2  shed  light  on  challenges,  difficulties, and limitations they encountered while applying the approach. additionally, the insights provided by the experienced practitioners allowed us to identify areas for improvement and potential enhancements. with the feedback received, we further refined perspecml v2 and came up to its final version. 


• rq4: to what extent do the practitioners perceive perspecml v2  as easy to 


use, useful and usable in the future? to address this research question, we pro- vided to participants a follow-up questionnaire. we collected feedback from both the novice and experienced practitioners regarding their perception of perspecml v2 as an approach for specifying ml-enabled systems. the novice practitioners, who used perspecml v2 during the requirements workshop session, provided their insights on the ease of use, usefulness, and usability of the approach. additionally, the experienced practitioners shared their opinions on the practicality and potential future utility of perspecml v2. by analyzing their feedback, we gained a compre- hensive understanding of how perspecml v2 was perceived by practitioners across different experience levels. 


 


7.3 selection of subjects 


the dynamic validation involved two main groups of participants from americanas: novice practitioners who specified two ml-enabled systems from scratch using per- specml v2, and experienced practitioners who also specified the systems, and additionally evaluated the resulting specifications. the practitioners were character- ized by having varied backgrounds, such as computer science, mathematics, physics, and others. the diversity in their educational background and experience helped validate the maturity  of  perspecml  v2.  their  feedback  shed  light  on  its  suitabil- ity for real-world implementation and if it meets the expectations and requirements of industry professionals. in table 19, we characterized the subjects by their role in the development of the ml-enabled systems involved in this study, educational background, and years of experience involved in ml projects. 


the subjects involved in specifying the ml-enabled systems from scratch were 


divided into two teams. in the fist one that we call team a, we had six novice practi- tioners and one experienced practitioner responsible for product classification system. in the second team that we call b, we had five novice practitioners and two expe- rienced practitioners responsible for market system. we highlighted the experienced practitioners who led each team with grey color in order to differentiate them from novice. note that experienced practitioners are data scientists with a different educa- tional background than computer science or engineering (except for p14), as expected for these positions [3, 28]. 


  


39 


table 19  subjects involved in the dynamic validation in industry 


 


team id role background experience 


(years) 


 p1  computer science 1 


 p2 p3 developer design computer science 


1 


1.5 


team a p4  computer engineering 1 


 p5 scrum master physics 1.5 


 p6 data scientist computer science 1 


 p7 data scientist linguistic 8 


 p8  electronic engineering 1 


 p9 


p10 developer computer engineering computer science 


1 1 


team b p11  mathematics 1 


 p12 scrum master computer science 2 


 p13 data scientist electrical engineering 4 


 p14 data scientist computer science 6 


 


7.4 data collection and analysis procedures 


to address the research questions outlined in this dynamic validation, we employed three main data collection procedures: requirements workshop sessions, interviews, and a follow-up questionnaire. 


 


7.4.1 requirements workshop sessions 


workshop design: we designed the requirements workshop sessions with a clear agenda and objectives, and outlined the tasks that the participants performed during the workshop, such as using perspecml v2 to specify the two ml-enabled systems from scratch. this allowed to provide the input to respond to rq1. 


data collection: during the sessions, we collected data in the form of written 


specifications produced by the practitioners. these specifications included concerns on the five perspectives such as objectives, user experience, infrastructure, model, and data. 


 


7.5 interviews 


interview design:  we  developed  a  semi-structured  interview  protocol  for  rq1. the protocol included a set of open-ended questions that focus on the experienced practitioners’ perception of the resulting specifications derived fromperspecml v2. questions explored aspects such as the quality, completeness, clarity, and effectiveness of the specifications. this shed light on answering rq1. 


data collection: we conducted interviews with the experienced practitioners. 


during the interviews, we used the protocol to guide the discussions, while allowing practitioners to share their thoughts and insights freely. we recorded the interviews in video format, with their consent, in order to ensure accurate capture of responses and allows for later review and analysis. 


data analysis: we transcribed the video recordings of the interviews into text 


format in order to analyze the participants’ responses, and then we applied coding 


  


40 


“at first sight, i was able to identify each perspective, its tasks, and their con- cerns. this helps me to better understand the requirements and dependencies of the product classification system” 


“i find the specification template and language constructs within perspecml beneficial in structuring the specifications effectively” 


“many times in our projects some of these concerns are only addressed when it is clearly too late. i see the diagram as a roadmap that allows me to identify components that would not be identified without its use” 


techniques to categorize them into themes. in addition, we triangulated by comparing and cross-referencing the results from the different interviewees. 


reporting: we summarized the findings and insights from the interviews in a 


structured manner by including direct quotes and paraphrased statements from the practitioners to support the analysis and interpretations. 


 


7.6 questionnaire 


questionnaire design: the questionnaire included structured questions and rating scales designed to capture quantitative and qualitative data related to rq2 and rq3, respectively. it addressed perceptions and feedback regarding the usefulness and ease 


of use of perspecml v2, and identified limitations or opportunities for improvement. 


data collection: the questionnaire responses were  collected  electronically 


through an online survey platform, taking care of anonymity and confidentiality. 


data analysis: quantitative data analysis techniques, such as descriptive statis- 


tics and inferential analysis, were used to analyze the questionnaire responses related to rq2. these findings provided numerical insights and trends, allowing for a compre- hensive understanding of participants’ perceptions about the acceptance of perspecml v2. qualitative data analysis techniques were also used to respond rq3, involving coding and categorization. 


 


7.7 results 


7.7.1 rq1. what perception do practitioners have while specifying 


ml-enabled systems by using perspecml v2 ? 


during the workshop specification sessions, we observed the interactions of practi- tioners with perspecml v2 to identify benefits or difficulties they encountered. the comments and discussions indicated that practitioners had a generally positive percep- tion of perspecml v2 as a supportive tool for guiding them through the specification process. for instance, novice practitioners p3 and p5 appreciated the visual and intuitive interface of perspecml v2 : 


 


 


as the workshops progressed, practitioners recognized the perspecml v2 ’s role 


in early identification and resolution of potential concerns in ml projects, and its abil- ity to facilitate collaboration and communication  among different teams involved in ml projects. p11, p13, p1 and p3 expressed: 


 


 


41 


“the specifications demonstrated a good understanding of the ml projects’ requirements, guiding the novice practitioners through the specification process” 


“the diagram can be extremely helpful for novice data scientists or engineers to get an overview of the ml workflow” 


“i am not sure if at the end the specifications are already sufficiently clear, but i can state what has been raised is reasonable and useful. coming up with a clear specification requires refinements and increments” 


“perspecml provides a more comprehensive overview and is far better than the ml canvas to support specifying ml-enabled systems” 


“currently, we use ml canvas to describe ml systems, but perspecml covers more elements, and helps analyze their relationships” 


 


 


 


 


 


while some initial learning curve was observed, practitioners quickly grasped per- 


specml v2 ’s functionalities and became comfortable using the approach. their perception of usability and effectiveness improved as they gained more hands-on experience during the workshop sessions. rq3 gave us more insights in this line. 


 


7.7.2 rq2. what perception do experienced practitioners have of 


the resulting specifications derived from perspecml v2 ? 


the experienced practitioners expressed positive feedback regarding the resulting spec- ifications derived from perspecml v2 for  the  two  ml  projects.  for  instance,  p13 and p14 appreciated the clear and well-structured nature  of the  specifications, and the utility for specific users: 


 


 


however, p7 pointed out minor areas where specifications could be further refined 


to better align with specific project needs: 


 


indeed, the requirements workshop was supposed to be the first effort towards 


comprehensive specifications that should be further improved after the workshop. on the other hand, p7 and p14 (experienced practitioners from separate workshops) both compared perspecml v2 with the approach they used so far in their projects. 


 


 


overall, the experienced practitioners were impressed with the novice practition- 


ers’ efforts and saw perspecml v2 as a valuable tool for fostering collaboration and understanding between different skill levels within the team. 


  


42 


“linking the model update task in the infrastructure perspective with the need to get user feedback in the user experience perspective makes sense. this encourages communication between teams involved in ml projects” 


“perspecml summarizes the work of several ml teams in one diagram” 


“there are several tasks that at the beginning of the project do not concern our team, but that deserve to be analyzed for their relationships with others” 


“based on my experience, ml systems can be expensive to maintain. even large companies should carefully consider the costs of maintaining ml systems before implementing them. i would include this concern for sure” 


“it is important to consider the versioning of the libraries that are typically used in the development of ml-enabled systems. on several occasions i have seen my teammates in trouble, for example, when the python version is not compatible with the tensorflow version. if there is a proper version management this could be avoided” 


“requirements specifications captures what the system is supposed to do, right? ml models tend to degrade over time due to several factors such as environmental and data changes. this behavior is typically not considered, therefore, it should be specified” 


“when analyzing the diagram i see that the number of concerns is considerable. that’s not a bad, in fact, it shows everything to think when designing ml systems. for this reason, i think it would be interesting to classify each concern by its importance. this would somehow prioritize the specification process” 


“it would be good to automate the approach by decreasing human involvement in the execution of perspecml that are prone to errors. it is a matter of practicality. in short, you can automate the perspecml’ logical flow” 


7.7.3 rq3. what are the limitations and opportunities for 


improvement of perspecml v2 ? 


the open-ended responses in the follow-up questionnaire provided valuable insights into the limitations and opportunities for improvement of perspecml v2. for instance, p7 suggested adding a concern related to the financial cost associated with the infras- tructure that is required to operate an ml-enabled system, while p3 recommended paying attention to the versioning of libraries. 


 


 


moreover, p13 suggested complementing the model perspective with the phe- 


nomenon that occurs when the performance of ml models decreases over time, and that both data scientists and customers typically pass up. 


 


on the other hand, p12 added another interesting opportunity for improvement: 


classifying the concerns by importance to better cope with the number of concerns to be analyzed. 


 


finally, p14 mentioned the importance of automating perspecml v2 : 


 


overall, the feedback indicated that perspecml v2 had potential for enhancement, 


and practitioners were eager to see future updates and features that could further elevate the tool’s usability and effectiveness. 


     


43 


7.7.4 rq4. to what extent do the practitioners perceive perspecml 


v2 as easy to use, useful and usable in the future? 


based on the tam questionnaire that included four-point likert scale ratings,  we found that practitioners indicated a high level of acceptance and positive perception of perspecml v2. the summary of the responses is shown in fig. 13. 


  


 


fig. 13 frequencies of the tam constructs for dynamic validation in industry 


  


the majority of participants rated perspecml v2 as easy to use, with a significant 


portion (12 out of 14) giving it a rating of 4 (strongly agree). the documentation, intuitive interface and clear instructions provided by perspecml v2 –improvements that came up in static validation–contributed to its perceived ease of use, making it accessible and user-friendly for both novice and experienced practitioners. however, one participant expressed partial disagreement with the statement of ease of use. this response came from p14, an experienced data scientist who mentioned suggestions for improvements on this topic in the previous question. 


additionally, the practitioners found perspecml v2 to be highly useful in the spec- 


ification process. excluding one who expressed partial agreement, all the participants gave it a rating of 4 for usefulness (strongly agree). indeed, the discussions and the outputs of the workshop sessions showed that perspecml v2  was especially valuable in guiding practitioners through the specification process and enhancing the overall clarity of the specifications. 


furthermore, the practitioners showed positive attitudes towards perspecml v2 ’s 


intended use. the majority of respondents (10 out of 14) expressed that they would be willing to use perspecml v2 in future ml projects, indicating the approach’s potential to become an essential part of their workflow for specifying ml-enabled systems. 


overall, the questionnaire results demonstrated a strong acceptance and positive 


perception of perspecml v2 ’s ease of use, usefulness, and future usability among the practitioners. when comparing these results with the static validation, we saw that the perception of ease of use improved considerably, indicating that the improvements from that evaluation had an effect. 


 


44 


11. we added ‘financial cost’ as a new concern of the infrastructure perspective, 


since ml typically demand implementing several services that impact project budget 


12. we added ‘versioning’ as a new concern of the model perspective, since this 


is essential for reproducibility, compatibility, and long-term maintainability of ml models 


13. we added ‘performance degradation’ as a new concern of the model per- 


spective, since it can lead to inaccurate predictions, which can cause problems for businesses and organizations 


14. based on a meta-review of the validations, we included ‘education  &  training’ 


in the user experience perspective, and ‘anonymization’ in the data perspec- tive. the first new concern will help that users have a clear understanding of the ml model’s capabilities and potential inaccuracies ensure the system’s credibil- ity and user satisfaction, and the second one will help to protect sensitive data when required while still maintaining the utility of the data for ml purposes 


15. we refined the perspecml v2 ’ logical flow to explicitly include the rele- 


vance of the concerns into desirable, important or essential. this could help to prioritize the requirements of ml-enabled systems 


at the end of this validation, we decided to consider the feedback of the practition- 


ers of the americanas company. in the following, we outline what was incorporated into perspecml v2 from this dynamic validation in industry, which led to the final version of perspecml. 


 


 


8 threats to validity 


assessing the validity of study results is particularly important for ensuring the accuracy, reliability, and generalization of findings. in this study, we empirically eval- uated perspecml by analyzing human factors, such as practitioners’ perceptions and experiences. in the following, we critically examine potential limitations and challenges that could impact the trustworthiness and applicability of our research outcomes. to this end, we followed the categories suggested by wohlin et al [50]. 


construct validity: for our quantitative and qualitative analyses, we conducted 


a mix of data collection methods, such as the tam questionnaire, focus groups, and interviews. these choices were based on the well-established theoretical foundation of such methods. for instance, the tam model has been widely used in technology acceptance research [44], and its questions were carefully designed to measure spe- cific constructs related to the users’ attitudes and intentions towards adopting our approach. 


internal validity: in the static  validation,  the  practitioners’  familiarity  with the 


ml projects that were retroactively specified may have influenced their perception and performance during the validation process, leading to potential bias in the results. to mitigate this threat, we decided to retroactively specify the ml projects with the support of the product owner of each project, but without involving the practitioners. in this case, we wanted to take advantage of this situation since by knowing the ml 


  


45 


projects, the practitioners could more easily evaluate the resulting specifications, e.g., whether important aspects was missing. 


external validity: we are aware that the generalization of the findings from the 


academic and static validation to real-world industrial scenarios may be limited. for instance, the toy scenario used in the academic setting and the specifications built retroactively in the static validation may not fully capture the complexity and chal- lenges faced in actual industrial projects. our intention with these artifacts was to use them to iteratively improve perspecml until it was mature and could be evaluated in a more realistic setting. regarding the subject representativeness, we believe that the validation conducted in academia with students, and in industry with novice and experienced practitioners, constitutes a diverse setting that allowed for the examina- tion of perspecml across different scenarios, thereby strengthening the generalization of the findings. 


conclusion validity: during the data collection and analysis procedures  of  the 


three evaluations, we used a single researcher for open coding. to mitigate this threat, we peer-reviewed the list of codes attached to the transcriptions, and validated our findings with the participants of the academic, static and dynamic validation. there- fore, as suggested by [29], we presented our conclusions to the involved participants to validate their agreement. moreover, we triangulated both qualitative and quantita- tive data helped provide a more robust understanding of perspecml’s usability and effectiveness, supporting well-informed conclusions. 


 


9 discussion 


in this section, we reflect on the outcomes of the validations and how they con- tribute to the understanding and improvement of perspecml, our perspective-based approach for specifying ml-enabled systems. we explore the broader implications of the findings, other areas of study, and how our approach can positively impact the development of ml-enabled systems. 


in terms of rigor, perspecml is the result of a series of validations that were 


conducted in different contexts, each contributing valuable insights and refining our approach to meet the diverse needs of practitioners involved in ml projects. through careful evaluations encompassing academia and industry, perspecml has undergone iterative enhancements, ensuring its effectiveness and adaptability in guiding the spec- ification of ml-enabled systems across various scenarios and project complexities. the combination of student validation, real-world discussions with experienced data scientists, and collaborative evaluations with both novice and experienced practition- ers has culminated in a robust and user-friendly approach that empowers teams to collaboratively and comprehensively define ml-enabled systems from inception to completion. 


in terms of scope and coverage, perspecml was designed with the underlying 


assumption that the problem to be solved can benefit from ml, which is not always the case. guidance to assess this assumption is out of our scope. while the focus of per- specml are requirements engineers, the specialists who provide a clear understanding of what needs to be built, other stakeholders such as project leaders can preside the 


  


46 


application of perspecml. in addition, we are aware that not every ml-enabled sys- tem needs to address all the concerns we proposed and not every ml-enabled system needs to implement them to the same degree. beyond qualities of ml components, of course, we also care about qualities of the system as a whole, including response time, safety, security, and usability. that is, traditional re for the entire system and its non-ml components is just as important. note that when considering the overall sys- tem, general quality characteristics of software products such as the ones mentioned in the iso/iec 25010 standard [24], should also be analyzed. 


in terms of expected benefits, the main purpose of perspecml is to support the 


specification of ml-enabled systems by analyzing the ml perspective-based diagram and filling out the ml specification template. nevertheless, we believe perspecml may eventually be useful in various situations. first, to validate an already specified ml- enabled system. in this case, the concerns would be a reference since they came from diverse source of knowledge (literature review, practical experiences and an external industrial experience on building ml-enabled systems [22]). second, perspecml may help design ml-enabled systems, since it includes (i) different components, including functional and non-functional properties, (ii) how they interact with each other, (iii) how they are deployed, and (iv) how they contribute with business requirements. third, perspecml is applicable to  the  most  common  ml  approaches  from  typical ml domains, such as classification or regression problems, to more complex domains, such as computer vision and natural language processing. in fact, in the validations we conducted, we used different type of ml domains. 


 


10 concluding remarks 


in this paper we presented perspecml, a perspective-based approach for specifying ml-enabled systems, designed to identify which attributes, including ml and non-ml, are important to contribute to the overall system’s quality. the approach empow- ers requirements engineers to analyze, with the support of business owners, domain experts, designers, software and ml engineers, and data scientists, 59 concerns related to typical tasks that such practitioners face, grouping them into five perspectives: system objectives, user experience, infrastructure, model, and data. 


we introduced two main artifacts of perspecml: (i) the  perspective-based  ml 


tasks and concern diagram that provides a holistic view of ml-enabled systems, and (ii) its corresponding specification template that provides a standardized format for documenting and organizing the applicable concerns. together, these artifacts serve to guide practitioners in collaboratively and comprehensively designing ml-enabled systems, enhancing their clarity, exploring trade-offs between conflicting requirements, uncovering hidden or overlooked requirements, and improving decision-making. 


the creation of perspecml involved a series of validations conducted in diverse 


contexts, encompassing both academic and real-world scenarios as suggested in [20] for scaling proposals up to practice. the evaluation process began with a validation in academia, where students from two courses of se for data science participated in specifying an ml-enabled system for a toy problem. this initial validation mainly showcased the promise of the approach and its potential for improvement in terms of 


  


47 


ease of use. the static validation in an industry setting involved discussions with prac- titioners of a r&d initiative, analyzing specifications retroactively for two ready-made ml projects. this validation highlighted perspecml’s role as a roadmap for identifying key components that could be missed without using the approach, but also identified opportunities for improvements related to usability. lastly, the dynamic validation engaged both novice and experienced practitioners of a brazilian large e-commerce company, who specified two real ml-enabled systems from scratch using perspecml. the feedback from previous validations allowed the practitioners to focus on improve- ments related to the completeness of the concerns and how to use the approach. as a result of the diverse evaluations and continuous improvements, perspecml stands as a promising approach, poised to positively impact the specification of ml-enabled systems. 


while the validations of perspecml have yielded promising results and provided 


valuable insights, there remain several avenues for future work and enhancements to further enrich the approach and its applications in the field. for instance, investigat- ing ways to automatically generate detailed documentation from the specifications provided in perspecml artifacts could significantly streamline project management and maintainability. this would further bridge the gap between specification and implementation phases. in addition, conducting other studies and soliciting continu- ous feedback from practitioners who actively use perspecml in real projects would offer valuable insights into its long-term benefits. by last, given the potentially con- flicting nature of the concerns highlighted in perspecml, delving into the study of trade-offs becomes even more promising, as it offers a pathway to address the complex particularities of ml-enabled systems. 


 


acknowledgment 


we would like to thank the employees of loggi, of the exacta initiative at puc-rio and of americanas s.a. thanks also for the financial support of the brazilian capes and cnpq agencies (grant 312827/2020-2). 


 

